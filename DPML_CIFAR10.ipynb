{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.layers import Input, InputLayer, Conv2D, Dense, Lambda, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras_vectorized import VectorizedDPKerasAdamOptimizer\n",
    "from tensorflow_privacy.privacy.analysis.compute_noise_from_budget_lib import compute_noise\n",
    "from tensorflow_privacy.privacy.keras_models.dp_keras_model import DPSequential\n",
    "\n",
    "\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import membership_inference_attack as mia\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackInputData\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import SlicingSpec\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackType\n",
    "\n",
    "import tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.plotting as plotting\n",
    "\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d5766",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 15e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7336b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def membership_inference_attack(model, X_train, X_test, y_train, y_test):\n",
    "    print('Predict on train...')\n",
    "    logits_train = model.predict(X_train, batch_size=batch_size)\n",
    "    print('Predict on test...')\n",
    "    logits_test = model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "    print('Apply softmax to get probabilities from logits...')\n",
    "    prob_train = special.softmax(logits_train, axis=1)\n",
    "    prob_test = special.softmax(logits_test, axis=1)\n",
    "\n",
    "    print('Compute losses...')\n",
    "    cce = tf.keras.backend.categorical_crossentropy\n",
    "    constant = tf.keras.backend.constant\n",
    "\n",
    "    loss_train = cce(constant(y_train), constant(prob_train), from_logits=False).numpy()\n",
    "    loss_test = cce(constant(y_test), constant(prob_test), from_logits=False).numpy()\n",
    "    \n",
    "    labels_train = np.argmax(y_train, axis=1)\n",
    "    labels_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    input = AttackInputData(\n",
    "      logits_train = logits_train,\n",
    "      logits_test = logits_test,\n",
    "      loss_train = loss_train,\n",
    "      loss_test = loss_test,\n",
    "      labels_train = labels_train,\n",
    "      labels_test = labels_test\n",
    "    )\n",
    "\n",
    "    # Run several attacks for different data slices\n",
    "    attacks_result = mia.run_attacks(input,\n",
    "                                     SlicingSpec(\n",
    "                                         entire_dataset = True,\n",
    "                                         by_class = True,\n",
    "                                         by_classification_correctness = True\n",
    "                                     ),\n",
    "                                     attack_types = [\n",
    "                                         AttackType.THRESHOLD_ATTACK,\n",
    "                                         AttackType.LOGISTIC_REGRESSION\n",
    "                                     ])\n",
    "\n",
    "    # Plot the ROC curve of the best classifier\n",
    "    fig = plotting.plot_roc_curve(\n",
    "        attacks_result.get_result_with_max_auc().roc_curve)\n",
    "\n",
    "    # Print a user-friendly summary of the attacks\n",
    "    print(attacks_result.summary(by_slices = True))\n",
    "    time.sleep(5)\n",
    "    return attacks_result.get_result_with_max_auc().get_auc(), attacks_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daea1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "cifar10_categories = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "(X_train, y_train_), (X_test, y_test_) = cifar10.load_data()\n",
    "X_train, X_test = X_train.reshape((X_train.shape[0], 32, 32, 3)), X_test.reshape((X_test.shape[0], 32, 32, 3))\n",
    "X_train_flat, X_test_flat = X_train.reshape(-1, 32*32*3), X_test.reshape(-1, 32*32*3)\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "y_train, y_test = np.eye(10)[y_train_.flatten()], np.eye(10)[y_test_.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(dropout=None, regularizer=None):\n",
    "    input_data = Input(shape = X_train[0].shape)\n",
    "    \n",
    "    x = Conv2D(20, (5,5), activation=\"relu\")(input_data)\n",
    "    x = MaxPooling2D()(x)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(x)\n",
    "    x = Conv2D(50, (5,5), activation=\"relu\")(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(500, activation=\"relu\", kernel_regularizer=regularizer)(x)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(x)\n",
    "    output = Dense(10)(x)\n",
    "\n",
    "    model = Model(input_data, output)\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "            \n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_dp_cnn(noise_multiplier, l2_norm_clip, microbatches):\n",
    "    input_data = Input(shape = X_train[0].shape)\n",
    "    x = Conv2D(20, (5,5), activation=\"relu\")(input_data)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(50, (5,5), activation=\"relu\")(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(500, activation=\"relu\")(x)\n",
    "    output = Dense(10)(x)\n",
    "    \n",
    "    model = Model(input_data, output)\n",
    "    \n",
    "    optimizer = DPKerasAdamOptimizer(\n",
    "                            l2_norm_clip=l2_norm_clip,\n",
    "                            noise_multiplier=noise_multiplier,\n",
    "                            num_microbatches=microbatches,\n",
    "                            learning_rate=learning_rate)\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f0824",
   "metadata": {},
   "source": [
    "# Baseline & anti-overfitting experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b5777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 100\n",
    "attacks = 5\n",
    "settings = [\n",
    "    (None,None),\n",
    "    (0.25,None),\n",
    "    (0.50,None),\n",
    "    (0.75,None),\n",
    "    (None,'l2'),\n",
    "    (0.25,'l2'),\n",
    "    (0.50,'l2'),\n",
    "    (0.75,'l2'),\n",
    "]\n",
    "results_summary = []\n",
    "\n",
    "for drop, reg in settings:\n",
    "    # Instantiate network\n",
    "    model = create_cnn(dropout=drop, regularizer=reg)\n",
    "    \n",
    "    # Train network until convergence\n",
    "    start_time = time.time()\n",
    "    r = model.fit(X_train, \n",
    "                y_train, \n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=epochs, \n",
    "                batch_size=batch_size\n",
    "               )\n",
    "    end_time = time.time()\n",
    "    time_elapsed = (end_time - start_time)\n",
    "\n",
    "    # MIA\n",
    "    aauc = []\n",
    "    aadv = []\n",
    "    for _ in range(attacks):\n",
    "        auc, adv = membership_inference_attack(model, X_train, X_test, y_train, y_test)\n",
    "        aauc.append(auc)\n",
    "        aadv.append(adv)\n",
    "    mauc = sum(aauc) / attacks\n",
    "    madv = sum(aadv) / attacks\n",
    "\n",
    "    # Write result summary\n",
    "    summ = ', '.join(map(str,[\n",
    "        len(r.history['loss']), #epochs\n",
    "        drop,\n",
    "        reg,\n",
    "        r.history['loss'][-1], \n",
    "        r.history['val_loss'][-1],\n",
    "        r.history['accuracy'][-1],\n",
    "        r.history['val_accuracy'][-1],\n",
    "        time_elapsed,\n",
    "        mauc,\n",
    "        madv\n",
    "    ]))\n",
    "    results_summary.append(summ)\n",
    "    print('='*40)\n",
    "\n",
    "print('Epochs, Dropout, Regularizer, Loss, Val loss, Accuracy, Val accuracy, Time, AUC, Advantage')\n",
    "for r in results_summary:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afcf78b",
   "metadata": {},
   "source": [
    "# Differential privacy experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8961c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST FOR DIFFERENT EPS\n",
    "results_summary = []\n",
    "\n",
    "n = X_train.shape[0]\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "microbatches = 10\n",
    "epsilons = [0.1,0.5,1,2,4,8,16,100,1000]\n",
    "delta = 1e-6\n",
    "min_noise = 1e-100\n",
    "l2_norm_clip = 2.5\n",
    "sampling_rate = batch_size / n\n",
    "attacks = 5\n",
    "\n",
    "for e in epsilons:\n",
    "    # Compute noise multiplier from target epsilon\n",
    "    noise_multiplier = compute_noise(n, batch_size, e, 47, delta, min_noise)\n",
    "    \n",
    "    # Instantiate network\n",
    "    model = create_dp_cnn(noise_multiplier, l2_norm_clip, microbatches)\n",
    "\n",
    "    # Train network\n",
    "    start_time = time.time()\n",
    "    r = model.fit(X_train, \n",
    "                 y_train, \n",
    "                 validation_data=(X_test, y_test), \n",
    "                 epochs=epochs, \n",
    "                 batch_size=batch_size,\n",
    "                 callbacks=[callback]\n",
    "                )\n",
    "    end_time = time.time()\n",
    "    time_elapsed = (end_time - start_time)\n",
    "    \n",
    "    # Compute epsilon\n",
    "    orders = [1 + x / 10. for x in range(1, 100)] + list(range(11, 101))\n",
    "    sampling_probability = batch_size / n\n",
    "    rdp = compute_rdp(q=sampling_probability,\n",
    "                    noise_multiplier=noise_multiplier,\n",
    "                    steps=len(r.history['loss']) * n // batch_size,\n",
    "                    orders=orders)\n",
    "    eps = get_privacy_spent(orders, rdp, target_delta=delta)\n",
    "    \n",
    "\n",
    "    # MIA \n",
    "    aauc = []\n",
    "    aadv = []\n",
    "    for _ in range(attacks):\n",
    "        auc, adv = membership_inference_attack(model, X_train, X_test, y_train, y_test)\n",
    "        aauc.append(auc)\n",
    "        aadv.append(adv)\n",
    "    mauc = sum(aauc) / attacks\n",
    "    madv = sum(aadv) / attacks\n",
    "\n",
    "    # Write result summary\n",
    "    summ = ', '.join(map(str,[\n",
    "          len(r.history['loss']),\n",
    "          e,\n",
    "          delta,\n",
    "          l2_norm_clip,\n",
    "          noise_multiplier,\n",
    "          sampling_rate,\n",
    "          eps[0],\n",
    "          r.history['loss'][-1], \n",
    "          r.history['val_loss'][-1],\n",
    "          r.history['accuracy'][-1],\n",
    "          r.history['val_accuracy'][-1],\n",
    "          time_elapsed,\n",
    "          mauc,\n",
    "          madv\n",
    "    ]))\n",
    "    results_summary.append(summ)\n",
    "    print('='*40)\n",
    "\n",
    "print('Epochs, Target epsilon, delta, C, Sigma, Sampling rate, Epsilon, Loss, Val loss, Accuracy, Val accuracy, Time, AUC, Advantage')\n",
    "for r in results_summary:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbeb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
