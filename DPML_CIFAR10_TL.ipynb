{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83691493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.layers import Input, InputLayer, Conv2D, Dense, Lambda, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer\n",
    "from tensorflow_privacy.privacy.analysis.compute_noise_from_budget_lib import compute_noise\n",
    "from tensorflow_privacy.privacy.keras_models.dp_keras_model import DPSequential\n",
    "\n",
    "\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import membership_inference_attack as mia\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackInputData\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import SlicingSpec\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackType\n",
    "\n",
    "import tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.plotting as plotting\n",
    "\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a7ffa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e558fa8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 15e-5\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469ea836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def membership_inference_attack(model, X_train, X_test, y_train, y_test):\n",
    "    print('Predict on train...')\n",
    "    logits_train = model.predict(X_train, batch_size=batch_size)\n",
    "    print('Predict on test...')\n",
    "    logits_test = model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "    print('Apply softmax to get probabilities from logits...')\n",
    "    prob_train = special.softmax(logits_train, axis=1)\n",
    "    prob_test = special.softmax(logits_test, axis=1)\n",
    "\n",
    "    print('Compute losses...')\n",
    "    cce = tf.keras.backend.categorical_crossentropy\n",
    "    constant = tf.keras.backend.constant\n",
    "\n",
    "    loss_train = cce(constant(y_train), constant(prob_train), from_logits=False).numpy()\n",
    "    loss_test = cce(constant(y_test), constant(prob_test), from_logits=False).numpy()\n",
    "    \n",
    "    labels_train = np.argmax(y_train, axis=1)\n",
    "    labels_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    input = AttackInputData(\n",
    "      logits_train = logits_train,\n",
    "      logits_test = logits_test,\n",
    "      loss_train = loss_train,\n",
    "      loss_test = loss_test,\n",
    "      labels_train = labels_train,\n",
    "      labels_test = labels_test\n",
    "    )\n",
    "\n",
    "    # Run several attacks for different data slices\n",
    "    attacks_result = mia.run_attacks(input,\n",
    "                                     SlicingSpec(\n",
    "                                         entire_dataset = True,\n",
    "                                         by_class = True,\n",
    "                                         by_classification_correctness = True\n",
    "                                     ),\n",
    "                                     attack_types = [\n",
    "                                         AttackType.THRESHOLD_ATTACK,\n",
    "                                         AttackType.LOGISTIC_REGRESSION,\n",
    "                                         AttackType.MULTI_LAYERED_PERCEPTRON,\n",
    "                                         AttackType.RANDOM_FOREST, \n",
    "                                         AttackType.K_NEAREST_NEIGHBORS,\n",
    "                                         AttackType.THRESHOLD_ENTROPY_ATTACK\n",
    "                                     ])\n",
    "\n",
    "    # Plot the ROC curve of the best classifier\n",
    "#     fig = plotting.plot_roc_curve(\n",
    "#         attacks_result.get_result_with_max_auc().roc_curve)\n",
    "\n",
    "    # Print a user-friendly summary of the attacks\n",
    "    print(attacks_result.summary(by_slices = True))\n",
    "    time.sleep(5)\n",
    "    return attacks_result.get_result_with_max_auc().get_auc(), attacks_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04828487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_extractor():\n",
    "    input_tensor = Input(shape=(32, 32, 3))\n",
    "    resized_images = Lambda(lambda image: tf.image.resize(image, (224, 224)))(input_tensor)\n",
    "    base_model = applications.DenseNet201(include_top=False,\n",
    "                                          weights='imagenet',\n",
    "                                          input_tensor=resized_images,\n",
    "                                          input_shape=(224, 224, 3),\n",
    "                                          pooling='max')\n",
    "    \n",
    "    output = base_model.layers[-1].output\n",
    "    base_model = Model(inputs=input_tensor, outputs=output)\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c118a1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "cifar10_categories = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "(X_train, y_train_), (X_test, y_test_) = cifar10.load_data()\n",
    "X_train_flat, X_test_flat = X_train.reshape(-1, 32*32*3), X_test.reshape(-1, 32*32*3)\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "y_train, y_test = np.eye(10)[y_train_.flatten()], np.eye(10)[y_test_.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c17e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = feature_extractor()\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "train_generator = train_datagen.flow(X_train,\n",
    "                                     y_train,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=False)\n",
    "features_train = feature_extractor.predict(train_generator)\n",
    "\n",
    "# repeat the same operation with the test data (here used for validation)\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "val_generator = val_datagen.flow(X_test,\n",
    "                                 y_test,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=False)\n",
    "features_valid = feature_extractor.predict(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "621d28d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_network(dropout=None, regularizer=None):\n",
    "    input_data = Input(shape=features_train.shape[1])\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(input_data)\n",
    "        x = Dense(256, activation='relu', kernel_regularizer=regularizer)(x)\n",
    "    else:\n",
    "        x = Dense(256, activation='relu', kernel_regularizer=regularizer)(input_data)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(x)\n",
    "    output = Dense(10, kernel_regularizer=regularizer)(x)\n",
    "        \n",
    "    model = Model(input_data, output)\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "            \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=loss,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_networkDP(noise_multiplier, l2_norm_clip, microbatches):\n",
    "    model = Sequential(\n",
    "        layers = [\n",
    "            Input(shape=features_train.shape[1]),\n",
    "            Dense(256, \n",
    "                  activation='relu',\n",
    "                  kernel_initializer=tf.keras.initializers.he_normal()),\n",
    "            Dense(10,\n",
    "                  kernel_initializer=tf.keras.initializers.he_normal())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    optimizer = DPKerasAdamOptimizer(\n",
    "                            l2_norm_clip=l2_norm_clip,\n",
    "                            noise_multiplier=noise_multiplier,\n",
    "                            num_microbatches=microbatches,\n",
    "                            learning_rate=learning_rate)\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10010933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.8307 - accuracy: 0.7200 - val_loss: 0.5978 - val_accuracy: 0.7951\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5120 - accuracy: 0.8235 - val_loss: 0.5215 - val_accuracy: 0.8192\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4443 - accuracy: 0.8461 - val_loss: 0.5012 - val_accuracy: 0.8295\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4012 - accuracy: 0.8616 - val_loss: 0.4755 - val_accuracy: 0.8404\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3708 - accuracy: 0.8726 - val_loss: 0.4636 - val_accuracy: 0.8443\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3438 - accuracy: 0.8813 - val_loss: 0.4622 - val_accuracy: 0.8421\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3202 - accuracy: 0.8892 - val_loss: 0.4479 - val_accuracy: 0.8511\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.2976 - accuracy: 0.8981 - val_loss: 0.4512 - val_accuracy: 0.8494\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.2787 - accuracy: 0.9053 - val_loss: 0.4678 - val_accuracy: 0.8449\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.2571 - accuracy: 0.9139 - val_loss: 0.4639 - val_accuracy: 0.8449\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.2382 - accuracy: 0.9211 - val_loss: 0.4676 - val_accuracy: 0.8456\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.2179 - accuracy: 0.9290 - val_loss: 0.4567 - val_accuracy: 0.8515\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.2007 - accuracy: 0.9356 - val_loss: 0.4820 - val_accuracy: 0.8486\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9415 - val_loss: 0.4675 - val_accuracy: 0.8479\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1674 - accuracy: 0.9484 - val_loss: 0.4793 - val_accuracy: 0.8489\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1519 - accuracy: 0.9551 - val_loss: 0.4732 - val_accuracy: 0.8501\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1398 - accuracy: 0.9601 - val_loss: 0.4913 - val_accuracy: 0.8490\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1225 - accuracy: 0.9679 - val_loss: 0.4767 - val_accuracy: 0.8526\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1103 - accuracy: 0.9724 - val_loss: 0.4859 - val_accuracy: 0.8528\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0979 - accuracy: 0.9764 - val_loss: 0.5051 - val_accuracy: 0.8511\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0881 - accuracy: 0.9800 - val_loss: 0.4998 - val_accuracy: 0.8533\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0768 - accuracy: 0.9848 - val_loss: 0.5245 - val_accuracy: 0.8474\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0690 - accuracy: 0.9868 - val_loss: 0.5178 - val_accuracy: 0.8524\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0593 - accuracy: 0.9907 - val_loss: 0.5290 - val_accuracy: 0.8516\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0526 - accuracy: 0.9924 - val_loss: 0.5437 - val_accuracy: 0.8516\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0453 - accuracy: 0.9945 - val_loss: 0.5551 - val_accuracy: 0.8500\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0400 - accuracy: 0.9959 - val_loss: 0.5759 - val_accuracy: 0.8494\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0347 - accuracy: 0.9970 - val_loss: 0.5750 - val_accuracy: 0.8470\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0294 - accuracy: 0.9979 - val_loss: 0.5839 - val_accuracy: 0.8507\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0251 - accuracy: 0.9985 - val_loss: 0.5960 - val_accuracy: 0.8519\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0227 - accuracy: 0.9989 - val_loss: 0.6073 - val_accuracy: 0.8488\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9994 - val_loss: 0.6305 - val_accuracy: 0.8472\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0172 - accuracy: 0.9993 - val_loss: 0.6325 - val_accuracy: 0.8525\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0156 - accuracy: 0.9995 - val_loss: 0.6454 - val_accuracy: 0.8477\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0127 - accuracy: 0.9996 - val_loss: 0.6557 - val_accuracy: 0.8507\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9993 - val_loss: 0.6748 - val_accuracy: 0.8492\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0093 - accuracy: 0.9999 - val_loss: 0.6830 - val_accuracy: 0.8530\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9999 - val_loss: 0.6999 - val_accuracy: 0.8516\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9965 - val_loss: 0.7018 - val_accuracy: 0.8532\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.7013 - val_accuracy: 0.8540\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7151 - val_accuracy: 0.8545\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.7250 - val_accuracy: 0.8553\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.7304 - val_accuracy: 0.8540\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.7489 - val_accuracy: 0.8561\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.7642 - val_accuracy: 0.8487\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0306 - accuracy: 0.9903 - val_loss: 0.7610 - val_accuracy: 0.8517\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7701 - val_accuracy: 0.8519\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.7722 - val_accuracy: 0.8549\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.7811 - val_accuracy: 0.8536\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7922 - val_accuracy: 0.8526\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.78 on slice CLASS=3\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.47 on slice CLASS=3\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  RANDOM_FOREST (with 10000 training and 10000 test examples) achieved an AUC of 0.69\n",
      "  RANDOM_FOREST (with 10000 training and 10000 test examples) achieved an advantage of 0.27\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.67\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.26\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.63\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.22\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.71\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.35\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.78\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.47\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.73\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.38\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.76\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.37\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.65\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.24\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.63\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.20\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.62\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.22\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.67\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.26\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  RANDOM_FOREST (with 8526 training and 8526 test examples) achieved an AUC of 0.62\n",
      "  RANDOM_FOREST (with 8526 training and 8526 test examples) achieved an advantage of 0.15\n",
      "========================================\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.3068 - accuracy: 0.5823 - val_loss: 0.6796 - val_accuracy: 0.7672\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.8003 - accuracy: 0.7219 - val_loss: 0.5866 - val_accuracy: 0.7974\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7129 - accuracy: 0.7542 - val_loss: 0.5593 - val_accuracy: 0.8108\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6614 - accuracy: 0.7729 - val_loss: 0.5289 - val_accuracy: 0.8165\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6238 - accuracy: 0.7818 - val_loss: 0.5098 - val_accuracy: 0.8246\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5979 - accuracy: 0.7920 - val_loss: 0.4991 - val_accuracy: 0.8292\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5808 - accuracy: 0.7972 - val_loss: 0.4879 - val_accuracy: 0.8341\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5648 - accuracy: 0.8035 - val_loss: 0.4886 - val_accuracy: 0.8357\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5516 - accuracy: 0.8084 - val_loss: 0.4717 - val_accuracy: 0.8384\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5364 - accuracy: 0.8093 - val_loss: 0.4723 - val_accuracy: 0.8398\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5223 - accuracy: 0.8162 - val_loss: 0.4581 - val_accuracy: 0.8427\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5113 - accuracy: 0.8183 - val_loss: 0.4508 - val_accuracy: 0.8451\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5038 - accuracy: 0.8226 - val_loss: 0.4536 - val_accuracy: 0.8452\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4948 - accuracy: 0.8254 - val_loss: 0.4479 - val_accuracy: 0.8455\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4869 - accuracy: 0.8288 - val_loss: 0.4446 - val_accuracy: 0.8481\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4803 - accuracy: 0.8295 - val_loss: 0.4387 - val_accuracy: 0.8498\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4726 - accuracy: 0.8323 - val_loss: 0.4391 - val_accuracy: 0.8500\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4660 - accuracy: 0.8356 - val_loss: 0.4374 - val_accuracy: 0.8510\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4573 - accuracy: 0.8373 - val_loss: 0.4380 - val_accuracy: 0.8512\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4515 - accuracy: 0.8403 - val_loss: 0.4350 - val_accuracy: 0.8520\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4455 - accuracy: 0.8408 - val_loss: 0.4306 - val_accuracy: 0.8539\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4428 - accuracy: 0.8444 - val_loss: 0.4201 - val_accuracy: 0.8552\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4373 - accuracy: 0.8456 - val_loss: 0.4237 - val_accuracy: 0.8547\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4291 - accuracy: 0.8469 - val_loss: 0.4228 - val_accuracy: 0.8561\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4247 - accuracy: 0.8504 - val_loss: 0.4234 - val_accuracy: 0.8575\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4207 - accuracy: 0.8512 - val_loss: 0.4173 - val_accuracy: 0.8581\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4179 - accuracy: 0.8523 - val_loss: 0.4167 - val_accuracy: 0.8577\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4061 - accuracy: 0.8570 - val_loss: 0.4171 - val_accuracy: 0.8585\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4041 - accuracy: 0.8569 - val_loss: 0.4167 - val_accuracy: 0.8577\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4008 - accuracy: 0.8580 - val_loss: 0.4116 - val_accuracy: 0.8599\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3937 - accuracy: 0.8599 - val_loss: 0.4104 - val_accuracy: 0.8616\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3929 - accuracy: 0.8596 - val_loss: 0.4097 - val_accuracy: 0.8615\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3889 - accuracy: 0.8616 - val_loss: 0.4106 - val_accuracy: 0.8600\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3841 - accuracy: 0.8636 - val_loss: 0.4147 - val_accuracy: 0.8586\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3858 - accuracy: 0.8632 - val_loss: 0.4113 - val_accuracy: 0.8596\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3774 - accuracy: 0.8646 - val_loss: 0.4105 - val_accuracy: 0.8592\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3765 - accuracy: 0.8662 - val_loss: 0.4126 - val_accuracy: 0.8588\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3696 - accuracy: 0.8678 - val_loss: 0.4050 - val_accuracy: 0.8617\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3668 - accuracy: 0.8686 - val_loss: 0.4027 - val_accuracy: 0.8619\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3575 - accuracy: 0.8738 - val_loss: 0.4033 - val_accuracy: 0.8616\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3610 - accuracy: 0.8728 - val_loss: 0.4118 - val_accuracy: 0.8598\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3647 - accuracy: 0.8682 - val_loss: 0.4085 - val_accuracy: 0.8625\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3563 - accuracy: 0.8737 - val_loss: 0.4048 - val_accuracy: 0.8600\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3527 - accuracy: 0.8737 - val_loss: 0.3998 - val_accuracy: 0.8611\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3491 - accuracy: 0.8752 - val_loss: 0.4007 - val_accuracy: 0.8641\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3461 - accuracy: 0.8759 - val_loss: 0.4073 - val_accuracy: 0.8583\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3419 - accuracy: 0.8784 - val_loss: 0.4000 - val_accuracy: 0.8622\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3440 - accuracy: 0.8750 - val_loss: 0.4038 - val_accuracy: 0.8600\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3415 - accuracy: 0.8766 - val_loss: 0.4003 - val_accuracy: 0.8600\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3389 - accuracy: 0.8791 - val_loss: 0.4021 - val_accuracy: 0.8627\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  RANDOM_FOREST (with 1373 training and 1373 test examples) achieved an AUC of 0.69 on slice CORRECTLY_CLASSIFIED=False\n",
      "  RANDOM_FOREST (with 1373 training and 1373 test examples) achieved an advantage of 0.31 on slice CORRECTLY_CLASSIFIED=False\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 10000 training and 10000 test examples) achieved an AUC of 0.57\n",
      "  LOGISTIC_REGRESSION (with 10000 training and 10000 test examples) achieved an advantage of 0.13\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.59\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.16\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.59\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.16\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.61\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.18\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.56\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.16\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.62\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.20\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  RANDOM_FOREST (with 8627 training and 8627 test examples) achieved an AUC of 0.53\n",
      "  RANDOM_FOREST (with 8627 training and 8627 test examples) achieved an advantage of 0.06\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  RANDOM_FOREST (with 1373 training and 1373 test examples) achieved an AUC of 0.69\n",
      "  RANDOM_FOREST (with 1373 training and 1373 test examples) achieved an advantage of 0.31\n",
      "========================================\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.2228 - accuracy: 0.3564 - val_loss: 1.1581 - val_accuracy: 0.6724\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.2992 - accuracy: 0.5450 - val_loss: 0.9266 - val_accuracy: 0.7335\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.1139 - accuracy: 0.6118 - val_loss: 0.7952 - val_accuracy: 0.7653\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.0198 - accuracy: 0.6447 - val_loss: 0.7466 - val_accuracy: 0.7797\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.9617 - accuracy: 0.6673 - val_loss: 0.7044 - val_accuracy: 0.7848\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.9217 - accuracy: 0.6797 - val_loss: 0.6901 - val_accuracy: 0.7950\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.8875 - accuracy: 0.6925 - val_loss: 0.6549 - val_accuracy: 0.7975\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.8663 - accuracy: 0.7007 - val_loss: 0.6296 - val_accuracy: 0.8060\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.8434 - accuracy: 0.7081 - val_loss: 0.6323 - val_accuracy: 0.8026\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.8302 - accuracy: 0.7149 - val_loss: 0.6128 - val_accuracy: 0.8082\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.8120 - accuracy: 0.7198 - val_loss: 0.5964 - val_accuracy: 0.8156\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7998 - accuracy: 0.7237 - val_loss: 0.5920 - val_accuracy: 0.8175\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7865 - accuracy: 0.7293 - val_loss: 0.5827 - val_accuracy: 0.8189\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7749 - accuracy: 0.7326 - val_loss: 0.5799 - val_accuracy: 0.8184\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7720 - accuracy: 0.7329 - val_loss: 0.5803 - val_accuracy: 0.8200\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7585 - accuracy: 0.7402 - val_loss: 0.5724 - val_accuracy: 0.8241\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7503 - accuracy: 0.7398 - val_loss: 0.5478 - val_accuracy: 0.8260\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7453 - accuracy: 0.7412 - val_loss: 0.5488 - val_accuracy: 0.8265\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7391 - accuracy: 0.7439 - val_loss: 0.5627 - val_accuracy: 0.8277\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7388 - accuracy: 0.7455 - val_loss: 0.5464 - val_accuracy: 0.8296\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7329 - accuracy: 0.7468 - val_loss: 0.5419 - val_accuracy: 0.8301\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7271 - accuracy: 0.7478 - val_loss: 0.5412 - val_accuracy: 0.8318\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7207 - accuracy: 0.7502 - val_loss: 0.5386 - val_accuracy: 0.8319\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7221 - accuracy: 0.7506 - val_loss: 0.5353 - val_accuracy: 0.8316\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7124 - accuracy: 0.7550 - val_loss: 0.5315 - val_accuracy: 0.8304\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7084 - accuracy: 0.7550 - val_loss: 0.5333 - val_accuracy: 0.8337\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7100 - accuracy: 0.7559 - val_loss: 0.5314 - val_accuracy: 0.8352\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7031 - accuracy: 0.7582 - val_loss: 0.5295 - val_accuracy: 0.8354\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7043 - accuracy: 0.7580 - val_loss: 0.5307 - val_accuracy: 0.8338\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6956 - accuracy: 0.7596 - val_loss: 0.5206 - val_accuracy: 0.8361\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6950 - accuracy: 0.7613 - val_loss: 0.5288 - val_accuracy: 0.8349\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6896 - accuracy: 0.7592 - val_loss: 0.5170 - val_accuracy: 0.8367\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.7637 - val_loss: 0.5188 - val_accuracy: 0.8393\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6846 - accuracy: 0.7607 - val_loss: 0.5214 - val_accuracy: 0.8389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6842 - accuracy: 0.7628 - val_loss: 0.5269 - val_accuracy: 0.8350\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6801 - accuracy: 0.7645 - val_loss: 0.5260 - val_accuracy: 0.8377\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6830 - accuracy: 0.7635 - val_loss: 0.5145 - val_accuracy: 0.8403\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6785 - accuracy: 0.7667 - val_loss: 0.5123 - val_accuracy: 0.8376\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6808 - accuracy: 0.7662 - val_loss: 0.5138 - val_accuracy: 0.8415\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6712 - accuracy: 0.7676 - val_loss: 0.5105 - val_accuracy: 0.8376\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6714 - accuracy: 0.7667 - val_loss: 0.5037 - val_accuracy: 0.8414\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6712 - accuracy: 0.7670 - val_loss: 0.5044 - val_accuracy: 0.8411\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6683 - accuracy: 0.7682 - val_loss: 0.5035 - val_accuracy: 0.8405\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6680 - accuracy: 0.7691 - val_loss: 0.5096 - val_accuracy: 0.8416\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.7692 - val_loss: 0.5018 - val_accuracy: 0.8388\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6668 - accuracy: 0.7701 - val_loss: 0.5066 - val_accuracy: 0.8423\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6634 - accuracy: 0.7703 - val_loss: 0.5121 - val_accuracy: 0.8388\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6621 - accuracy: 0.7705 - val_loss: 0.5017 - val_accuracy: 0.8414\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.7751 - val_loss: 0.4971 - val_accuracy: 0.8412\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6560 - accuracy: 0.7726 - val_loss: 0.5066 - val_accuracy: 0.8427\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-97b5b6edc9de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0maadv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mauc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmembership_inference_attack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0maauc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0maadv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0d7d910023f6>\u001b[0m in \u001b[0;36mmembership_inference_attack\u001b[1;34m(model, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Run several attacks for different data slices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     attacks_result = mia.run_attacks(input,\n\u001b[0m\u001b[0;32m     32\u001b[0m                                      SlicingSpec(\n\u001b[0;32m     33\u001b[0m                                          \u001b[0mentire_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_privacy\\privacy\\privacy_tests\\membership_inference_attack\\membership_inference_attack.py\u001b[0m in \u001b[0;36mrun_attacks\u001b[1;34m(attack_input, slicing_spec, attack_types, privacy_report_metadata, balance_attacker_training, min_num_samples)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[0mattack_input_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattack_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingle_slice_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mattack_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattack_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m       attack_result = _run_attack(attack_input_slice, attack_type,\n\u001b[0m\u001b[0;32m    204\u001b[0m                                   \u001b[0mbalance_attacker_training\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                                   min_num_samples)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_privacy\\privacy\\privacy_tests\\membership_inference_attack\\membership_inference_attack.py\u001b[0m in \u001b[0;36m_run_attack\u001b[1;34m(attack_input, attack_type, balance_attacker_training, min_num_samples)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mattack_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_trained_attack\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     return _run_trained_attack(attack_input, attack_type,\n\u001b[0m\u001b[0;32m    159\u001b[0m                                balance_attacker_training)\n\u001b[0;32m    160\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mattack_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mAttackType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTHRESHOLD_ENTROPY_ATTACK\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_privacy\\privacy\\privacy_tests\\membership_inference_attack\\membership_inference_attack.py\u001b[0m in \u001b[0;36m_run_trained_attack\u001b[1;34m(attack_input, attack_type, balance_attacker_training)\u001b[0m\n\u001b[0;32m     67\u001b[0m       attack_input, balance=balance_attacker_training)\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m   attacker.train_model(prepared_attacker_data.features_train,\n\u001b[0m\u001b[0;32m     70\u001b[0m                        prepared_attacker_data.is_training_labels_train)\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_privacy\\privacy\\privacy_tests\\membership_inference_attack\\models.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, input_features, is_training_labels)\u001b[0m\n\u001b[0;32m    193\u001b[0m     model = model_selection.GridSearchCV(\n\u001b[0;32m    194\u001b[0m         rf_model, param_grid=param_grid, cv=3, n_jobs=n_jobs, verbose=0)\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 100\n",
    "attacks = 1\n",
    "settings = [\n",
    "    (None,None),\n",
    "    (0.25,None),\n",
    "    (0.50,None),\n",
    "    (0.75,None),\n",
    "    (None,'l2'),\n",
    "    (0.25,'l2'),\n",
    "    (0.50,'l2'),\n",
    "    (0.75,'l2'),\n",
    "]\n",
    "results_summary = []\n",
    "\n",
    "for drop, reg in settings: \n",
    "    # Instantiate network\n",
    "    classifier = create_network(dropout=drop, regularizer=reg)\n",
    "    \n",
    "    # Train network until convergence    \n",
    "    start_time = time.time()\n",
    "    r = classifier.fit(features_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(features_valid, y_test),\n",
    "                    shuffle=True,\n",
    "                    verbose=1\n",
    "                   )\n",
    "    end_time = time.time()\n",
    "    model = Sequential([feature_extractor, classifier])\n",
    "    model.compile(loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
    "    time_elapsed = (end_time - start_time)\n",
    "    \n",
    "    # MIA\n",
    "    aauc = []\n",
    "    aadv = []\n",
    "    for _ in range(attacks):\n",
    "        auc, adv = membership_inference_attack(model, X_train, X_test, y_train, y_test)\n",
    "        aauc.append(auc)\n",
    "        aadv.append(adv)\n",
    "    mauc = sum(aauc) / attacks\n",
    "    madv = sum(aadv) / attacks\n",
    "\n",
    "    # Write result summary\n",
    "    summ = ', '.join(map(str,[\n",
    "        len(r.history['loss']), #epochs\n",
    "        drop,\n",
    "        reg,\n",
    "        r.history['loss'][-1], \n",
    "        r.history['val_loss'][-1],\n",
    "        r.history['accuracy'][-1],\n",
    "        r.history['val_accuracy'][-1],\n",
    "        time_elapsed,\n",
    "        mauc,\n",
    "        madv\n",
    "    ]))\n",
    "\n",
    "    results_summary.append(summ)\n",
    "    print('='*40)\n",
    "    \n",
    "print('Epochs, Dropout, Regularizer, Loss, Val loss, Accuracy, Val accuracy, Time, AUC, Advantage')\n",
    "for r in results_summary:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63417f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.2% and noise_multiplier = 13.133789235896893 iterated over 25000 steps satisfies differential privacy with eps = 0.1 and delta = 1e-06.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 34s 66ms/step - loss: 4.1917 - accuracy: 0.1249 - val_loss: 3.2711 - val_accuracy: 0.1460\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 3.1037 - accuracy: 0.1560 - val_loss: 2.9478 - val_accuracy: 0.1727\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 2.8494 - accuracy: 0.1827 - val_loss: 2.7261 - val_accuracy: 0.2022\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 2.6689 - accuracy: 0.2100 - val_loss: 2.5857 - val_accuracy: 0.2309\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 2.5520 - accuracy: 0.2382 - val_loss: 2.4776 - val_accuracy: 0.2561\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 2.4293 - accuracy: 0.2672 - val_loss: 2.3389 - val_accuracy: 0.2902\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 31s 63ms/step - loss: 2.2871 - accuracy: 0.2990 - val_loss: 2.2219 - val_accuracy: 0.3176\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 2.1865 - accuracy: 0.3235 - val_loss: 2.1378 - val_accuracy: 0.3402\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 2.0962 - accuracy: 0.3467 - val_loss: 2.0530 - val_accuracy: 0.3599\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 2.0233 - accuracy: 0.3679 - val_loss: 1.9824 - val_accuracy: 0.3783\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 1.9541 - accuracy: 0.3846 - val_loss: 1.9065 - val_accuracy: 0.3981\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.8769 - accuracy: 0.4079 - val_loss: 1.8392 - val_accuracy: 0.4167\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 1.8191 - accuracy: 0.4305 - val_loss: 1.7968 - val_accuracy: 0.4369\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.7821 - accuracy: 0.4465 - val_loss: 1.7401 - val_accuracy: 0.4567\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.7289 - accuracy: 0.4630 - val_loss: 1.7068 - val_accuracy: 0.4684\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.7031 - accuracy: 0.4740 - val_loss: 1.6826 - val_accuracy: 0.4816\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.6816 - accuracy: 0.4870 - val_loss: 1.6652 - val_accuracy: 0.4930\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 1.6518 - accuracy: 0.4970 - val_loss: 1.6289 - val_accuracy: 0.5042\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 1.6191 - accuracy: 0.5086 - val_loss: 1.5942 - val_accuracy: 0.5182\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 1.5777 - accuracy: 0.5204 - val_loss: 1.5598 - val_accuracy: 0.5261\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.5522 - accuracy: 0.5312 - val_loss: 1.5318 - val_accuracy: 0.5359\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.5147 - accuracy: 0.5429 - val_loss: 1.4966 - val_accuracy: 0.5468\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.4864 - accuracy: 0.5517 - val_loss: 1.4722 - val_accuracy: 0.5548\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.4521 - accuracy: 0.5607 - val_loss: 1.4419 - val_accuracy: 0.5605\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.4410 - accuracy: 0.5655 - val_loss: 1.4326 - val_accuracy: 0.5694\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.4147 - accuracy: 0.5735 - val_loss: 1.4026 - val_accuracy: 0.5791\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.3982 - accuracy: 0.5808 - val_loss: 1.3962 - val_accuracy: 0.5837\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.3878 - accuracy: 0.5861 - val_loss: 1.3746 - val_accuracy: 0.5893\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.3709 - accuracy: 0.5909 - val_loss: 1.3609 - val_accuracy: 0.5933\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 31s 63ms/step - loss: 1.3549 - accuracy: 0.5963 - val_loss: 1.3501 - val_accuracy: 0.5999\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.3424 - accuracy: 0.6010 - val_loss: 1.3415 - val_accuracy: 0.6047\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.3462 - accuracy: 0.6035 - val_loss: 1.3490 - val_accuracy: 0.6056\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.3334 - accuracy: 0.6075 - val_loss: 1.3321 - val_accuracy: 0.6086\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.3208 - accuracy: 0.6109 - val_loss: 1.3146 - val_accuracy: 0.6105\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.3092 - accuracy: 0.6144 - val_loss: 1.3048 - val_accuracy: 0.6154\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.3019 - accuracy: 0.6176 - val_loss: 1.3073 - val_accuracy: 0.6187\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.3005 - accuracy: 0.6204 - val_loss: 1.3095 - val_accuracy: 0.6194\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.3058 - accuracy: 0.6220 - val_loss: 1.3138 - val_accuracy: 0.6226\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.2979 - accuracy: 0.6261 - val_loss: 1.2995 - val_accuracy: 0.6262\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.2821 - accuracy: 0.6291 - val_loss: 1.2895 - val_accuracy: 0.6304\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 1.2761 - accuracy: 0.6323 - val_loss: 1.2850 - val_accuracy: 0.6333\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.2818 - accuracy: 0.6341 - val_loss: 1.2886 - val_accuracy: 0.6346\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.2704 - accuracy: 0.6370 - val_loss: 1.2744 - val_accuracy: 0.6371\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.2576 - accuracy: 0.6398 - val_loss: 1.2685 - val_accuracy: 0.6408\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.2523 - accuracy: 0.6434 - val_loss: 1.2619 - val_accuracy: 0.6437\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.2460 - accuracy: 0.6452 - val_loss: 1.2580 - val_accuracy: 0.6452\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.2372 - accuracy: 0.6477 - val_loss: 1.2405 - val_accuracy: 0.6464\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.2309 - accuracy: 0.6513 - val_loss: 1.2547 - val_accuracy: 0.6471\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.2406 - accuracy: 0.6510 - val_loss: 1.2468 - val_accuracy: 0.6510\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.2391 - accuracy: 0.6535 - val_loss: 1.2499 - val_accuracy: 0.6536\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.56 on slice CLASS=0\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.11 on slice CLASS=0\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 10000 training and 10000 test examples) achieved an AUC of 0.50\n",
      "  LOGISTIC_REGRESSION (with 10000 training and 10000 test examples) achieved an advantage of 0.02\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.56\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  THRESHOLD_ENTROPY_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.06\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  THRESHOLD_ENTROPY_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.06\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  RANDOM_FOREST (with 6536 training and 6536 test examples) achieved an AUC of 0.51\n",
      "  MULTI_LAYERED_PERCEPTRON (with 6536 training and 6536 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  K_NEAREST_NEIGHBORS (with 3464 training and 3464 test examples) achieved an AUC of 0.51\n",
      "  K_NEAREST_NEIGHBORS (with 3464 training and 3464 test examples) achieved an advantage of 0.05\n",
      "========================================\n",
      "DP-SGD with sampling rate = 0.2% and noise_multiplier = 2.8456245157566924 iterated over 25000 steps satisfies differential privacy with eps = 0.5 and delta = 1e-06.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 3.2102 - accuracy: 0.1437 - val_loss: 2.5984 - val_accuracy: 0.2186\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 2.2596 - accuracy: 0.2915 - val_loss: 1.9985 - val_accuracy: 0.3642\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.7983 - accuracy: 0.4151 - val_loss: 1.6578 - val_accuracy: 0.4593\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.5283 - accuracy: 0.4993 - val_loss: 1.4591 - val_accuracy: 0.5268\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.3676 - accuracy: 0.5548 - val_loss: 1.3369 - val_accuracy: 0.5729\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.2617 - accuracy: 0.5912 - val_loss: 1.2554 - val_accuracy: 0.6071\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.1899 - accuracy: 0.6187 - val_loss: 1.1838 - val_accuracy: 0.6304\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.1332 - accuracy: 0.6412 - val_loss: 1.1558 - val_accuracy: 0.6462\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 1.1046 - accuracy: 0.6563 - val_loss: 1.1321 - val_accuracy: 0.6610\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.0832 - accuracy: 0.6693 - val_loss: 1.1126 - val_accuracy: 0.6677\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 1.0629 - accuracy: 0.6786 - val_loss: 1.1019 - val_accuracy: 0.6783\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.0461 - accuracy: 0.6875 - val_loss: 1.0850 - val_accuracy: 0.6859\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.0313 - accuracy: 0.6946 - val_loss: 1.0797 - val_accuracy: 0.6934\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.0196 - accuracy: 0.7016 - val_loss: 1.0712 - val_accuracy: 0.6979\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.0063 - accuracy: 0.7073 - val_loss: 1.0635 - val_accuracy: 0.7010\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.0001 - accuracy: 0.7117 - val_loss: 1.0619 - val_accuracy: 0.7051\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9979 - accuracy: 0.7163 - val_loss: 1.0656 - val_accuracy: 0.7095\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9911 - accuracy: 0.7201 - val_loss: 1.0585 - val_accuracy: 0.7115\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9852 - accuracy: 0.7242 - val_loss: 1.0504 - val_accuracy: 0.7149\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9809 - accuracy: 0.7284 - val_loss: 1.0382 - val_accuracy: 0.7195\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9677 - accuracy: 0.7327 - val_loss: 1.0341 - val_accuracy: 0.7203\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9641 - accuracy: 0.7355 - val_loss: 1.0298 - val_accuracy: 0.7254\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9572 - accuracy: 0.7386 - val_loss: 1.0271 - val_accuracy: 0.7288\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9573 - accuracy: 0.7395 - val_loss: 1.0237 - val_accuracy: 0.7305\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9552 - accuracy: 0.7420 - val_loss: 1.0201 - val_accuracy: 0.7309\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9509 - accuracy: 0.7446 - val_loss: 1.0187 - val_accuracy: 0.7348\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9508 - accuracy: 0.7454 - val_loss: 1.0175 - val_accuracy: 0.7368\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.9480 - accuracy: 0.7479 - val_loss: 1.0183 - val_accuracy: 0.7379\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9489 - accuracy: 0.7495 - val_loss: 1.0194 - val_accuracy: 0.7370\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9469 - accuracy: 0.7510 - val_loss: 1.0143 - val_accuracy: 0.7394\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9474 - accuracy: 0.7521 - val_loss: 1.0161 - val_accuracy: 0.7393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9501 - accuracy: 0.7537 - val_loss: 1.0176 - val_accuracy: 0.7423\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9498 - accuracy: 0.7556 - val_loss: 1.0200 - val_accuracy: 0.7432\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9514 - accuracy: 0.7567 - val_loss: 1.0179 - val_accuracy: 0.7439\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9449 - accuracy: 0.7585 - val_loss: 1.0167 - val_accuracy: 0.7457\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9488 - accuracy: 0.7584 - val_loss: 1.0215 - val_accuracy: 0.7481\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9500 - accuracy: 0.7604 - val_loss: 1.0203 - val_accuracy: 0.7483\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9514 - accuracy: 0.7617 - val_loss: 1.0199 - val_accuracy: 0.7495\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9494 - accuracy: 0.7627 - val_loss: 1.0224 - val_accuracy: 0.7486\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9482 - accuracy: 0.7625 - val_loss: 1.0182 - val_accuracy: 0.7502\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.9494 - accuracy: 0.7635 - val_loss: 1.0258 - val_accuracy: 0.7509\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9491 - accuracy: 0.7650 - val_loss: 1.0259 - val_accuracy: 0.7525\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.9475 - accuracy: 0.7662 - val_loss: 1.0264 - val_accuracy: 0.7522\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9464 - accuracy: 0.7670 - val_loss: 1.0201 - val_accuracy: 0.7541\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9455 - accuracy: 0.7677 - val_loss: 1.0255 - val_accuracy: 0.7520\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9455 - accuracy: 0.7678 - val_loss: 1.0186 - val_accuracy: 0.7545\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9439 - accuracy: 0.7701 - val_loss: 1.0172 - val_accuracy: 0.7562\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.9423 - accuracy: 0.7700 - val_loss: 1.0186 - val_accuracy: 0.7553\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9390 - accuracy: 0.7719 - val_loss: 1.0115 - val_accuracy: 0.7565\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9387 - accuracy: 0.7723 - val_loss: 1.0134 - val_accuracy: 0.7578\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.55 on slice CLASS=9\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.12 on slice CLASS=9\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  LOGISTIC_REGRESSION (with 10000 training and 10000 test examples) achieved an AUC of 0.51\n",
      "  RANDOM_FOREST (with 10000 training and 10000 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.07\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  THRESHOLD_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.07\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  THRESHOLD_ENTROPY_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.50\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  LOGISTIC_REGRESSION (with 7578 training and 7578 test examples) achieved an AUC of 0.52\n",
      "  LOGISTIC_REGRESSION (with 7578 training and 7578 test examples) achieved an advantage of 0.06\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  K_NEAREST_NEIGHBORS (with 2422 training and 2422 test examples) achieved an AUC of 0.51\n",
      "  RANDOM_FOREST (with 2422 training and 2422 test examples) achieved an advantage of 0.09\n",
      "========================================\n",
      "DP-SGD with sampling rate = 0.2% and noise_multiplier = 1.6022653845391852 iterated over 25000 steps satisfies differential privacy with eps = 1 and delta = 1e-06.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 3.1851 - accuracy: 0.1701 - val_loss: 2.2284 - val_accuracy: 0.3116\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 1.8862 - accuracy: 0.4014 - val_loss: 1.6130 - val_accuracy: 0.4759\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 1.4690 - accuracy: 0.5289 - val_loss: 1.3360 - val_accuracy: 0.5727\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.2710 - accuracy: 0.5978 - val_loss: 1.2005 - val_accuracy: 0.6259\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.1518 - accuracy: 0.6402 - val_loss: 1.1116 - val_accuracy: 0.6592\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 1.0874 - accuracy: 0.6666 - val_loss: 1.0773 - val_accuracy: 0.6778\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.0480 - accuracy: 0.6846 - val_loss: 1.0414 - val_accuracy: 0.6921\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 1.0169 - accuracy: 0.6975 - val_loss: 1.0170 - val_accuracy: 0.7030\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9876 - accuracy: 0.7085 - val_loss: 1.0029 - val_accuracy: 0.7123\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9710 - accuracy: 0.7187 - val_loss: 0.9823 - val_accuracy: 0.7228\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9559 - accuracy: 0.7257 - val_loss: 0.9707 - val_accuracy: 0.7278\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9426 - accuracy: 0.7335 - val_loss: 0.9641 - val_accuracy: 0.7345\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9320 - accuracy: 0.7387 - val_loss: 0.9533 - val_accuracy: 0.7380\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9209 - accuracy: 0.7432 - val_loss: 0.9488 - val_accuracy: 0.7426\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9134 - accuracy: 0.7480 - val_loss: 0.9461 - val_accuracy: 0.7434\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9070 - accuracy: 0.7523 - val_loss: 0.9420 - val_accuracy: 0.7472\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9054 - accuracy: 0.7560 - val_loss: 0.9339 - val_accuracy: 0.7513\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9017 - accuracy: 0.7581 - val_loss: 0.9334 - val_accuracy: 0.7539\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8976 - accuracy: 0.7611 - val_loss: 0.9270 - val_accuracy: 0.7583\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8941 - accuracy: 0.7633 - val_loss: 0.9231 - val_accuracy: 0.7624\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8903 - accuracy: 0.7659 - val_loss: 0.9233 - val_accuracy: 0.7634\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8851 - accuracy: 0.7691 - val_loss: 0.9239 - val_accuracy: 0.7657\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8833 - accuracy: 0.7710 - val_loss: 0.9276 - val_accuracy: 0.7655\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8828 - accuracy: 0.7728 - val_loss: 0.9236 - val_accuracy: 0.7675\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8849 - accuracy: 0.7747 - val_loss: 0.9290 - val_accuracy: 0.7684\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8856 - accuracy: 0.7760 - val_loss: 0.9279 - val_accuracy: 0.7696\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8816 - accuracy: 0.7769 - val_loss: 0.9215 - val_accuracy: 0.7725\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8783 - accuracy: 0.7783 - val_loss: 0.9186 - val_accuracy: 0.7725\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8764 - accuracy: 0.7794 - val_loss: 0.9210 - val_accuracy: 0.7739\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8773 - accuracy: 0.7795 - val_loss: 0.9215 - val_accuracy: 0.7737\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8774 - accuracy: 0.7812 - val_loss: 0.9187 - val_accuracy: 0.7763\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8737 - accuracy: 0.7834 - val_loss: 0.9181 - val_accuracy: 0.7778\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8734 - accuracy: 0.7850 - val_loss: 0.9205 - val_accuracy: 0.7766\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8699 - accuracy: 0.7871 - val_loss: 0.9153 - val_accuracy: 0.7800\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8698 - accuracy: 0.7874 - val_loss: 0.9184 - val_accuracy: 0.7820\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8668 - accuracy: 0.7883 - val_loss: 0.9210 - val_accuracy: 0.7811\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8669 - accuracy: 0.7889 - val_loss: 0.9157 - val_accuracy: 0.7836\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8644 - accuracy: 0.7903 - val_loss: 0.9121 - val_accuracy: 0.7830\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8639 - accuracy: 0.7919 - val_loss: 0.9180 - val_accuracy: 0.7837\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8644 - accuracy: 0.7924 - val_loss: 0.9150 - val_accuracy: 0.7842\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8622 - accuracy: 0.7936 - val_loss: 0.9193 - val_accuracy: 0.7851\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8632 - accuracy: 0.7939 - val_loss: 0.9225 - val_accuracy: 0.7867\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8674 - accuracy: 0.7941 - val_loss: 0.9243 - val_accuracy: 0.7861\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8679 - accuracy: 0.7957 - val_loss: 0.9267 - val_accuracy: 0.7874\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8665 - accuracy: 0.7955 - val_loss: 0.9210 - val_accuracy: 0.7874\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8653 - accuracy: 0.7961 - val_loss: 0.9305 - val_accuracy: 0.7884\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8695 - accuracy: 0.7960 - val_loss: 0.9313 - val_accuracy: 0.7886\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8695 - accuracy: 0.7968 - val_loss: 0.9269 - val_accuracy: 0.7907\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8722 - accuracy: 0.7973 - val_loss: 0.9246 - val_accuracy: 0.7919\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8719 - accuracy: 0.7979 - val_loss: 0.9253 - val_accuracy: 0.7911\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.56 on slice CLASS=4\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.16 on slice CLASS=4\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 10000 training and 10000 test examples) achieved an AUC of 0.52\n",
      "  MULTI_LAYERED_PERCEPTRON (with 10000 training and 10000 test examples) achieved an advantage of 0.04\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  THRESHOLD_ENTROPY_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.56\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.13\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.56\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.16\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  THRESHOLD_ENTROPY_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.06\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  LOGISTIC_REGRESSION (with 7911 training and 7911 test examples) achieved an AUC of 0.51\n",
      "  LOGISTIC_REGRESSION (with 7911 training and 7911 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  THRESHOLD_ENTROPY_ATTACK (with 10087 training and 2089 test examples) achieved an AUC of 0.51\n",
      "  K_NEAREST_NEIGHBORS (with 2089 training and 2089 test examples) achieved an advantage of 0.05\n",
      "========================================\n",
      "DP-SGD with sampling rate = 0.2% and noise_multiplier = 1.013450319287938 iterated over 25000 steps satisfies differential privacy with eps = 2 and delta = 1e-06.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 2.5279 - accuracy: 0.2969 - val_loss: 1.7264 - val_accuracy: 0.4677\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.4457 - accuracy: 0.5461 - val_loss: 1.2939 - val_accuracy: 0.5975\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.1853 - accuracy: 0.6362 - val_loss: 1.1372 - val_accuracy: 0.6546\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 1.0781 - accuracy: 0.6787 - val_loss: 1.0712 - val_accuracy: 0.6862\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.0225 - accuracy: 0.7034 - val_loss: 1.0387 - val_accuracy: 0.7066\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.9853 - accuracy: 0.7211 - val_loss: 1.0030 - val_accuracy: 0.7198\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9541 - accuracy: 0.7323 - val_loss: 0.9862 - val_accuracy: 0.7272\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9377 - accuracy: 0.7418 - val_loss: 0.9674 - val_accuracy: 0.7392\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9224 - accuracy: 0.7503 - val_loss: 0.9605 - val_accuracy: 0.7458\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9112 - accuracy: 0.7553 - val_loss: 0.9577 - val_accuracy: 0.7482\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9044 - accuracy: 0.7601 - val_loss: 0.9447 - val_accuracy: 0.7549\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8951 - accuracy: 0.7648 - val_loss: 0.9423 - val_accuracy: 0.7584\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 0.8885 - accuracy: 0.7678 - val_loss: 0.9323 - val_accuracy: 0.7619\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8804 - accuracy: 0.7731 - val_loss: 0.9306 - val_accuracy: 0.7650\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8807 - accuracy: 0.7759 - val_loss: 0.9294 - val_accuracy: 0.7687\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8791 - accuracy: 0.7787 - val_loss: 0.9284 - val_accuracy: 0.7722\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8745 - accuracy: 0.7817 - val_loss: 0.9333 - val_accuracy: 0.7717\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8772 - accuracy: 0.7835 - val_loss: 0.9310 - val_accuracy: 0.7755\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8751 - accuracy: 0.7851 - val_loss: 0.9325 - val_accuracy: 0.7761\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8712 - accuracy: 0.7865 - val_loss: 0.9294 - val_accuracy: 0.7775\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8687 - accuracy: 0.7889 - val_loss: 0.9319 - val_accuracy: 0.7789\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8690 - accuracy: 0.7900 - val_loss: 0.9285 - val_accuracy: 0.7803\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8666 - accuracy: 0.7913 - val_loss: 0.9263 - val_accuracy: 0.7806\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8625 - accuracy: 0.7929 - val_loss: 0.9233 - val_accuracy: 0.7821\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8586 - accuracy: 0.7951 - val_loss: 0.9254 - val_accuracy: 0.7835\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8570 - accuracy: 0.7959 - val_loss: 0.9267 - val_accuracy: 0.7823\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8561 - accuracy: 0.7964 - val_loss: 0.9189 - val_accuracy: 0.7862\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8534 - accuracy: 0.7982 - val_loss: 0.9223 - val_accuracy: 0.7856\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8543 - accuracy: 0.7994 - val_loss: 0.9213 - val_accuracy: 0.7876\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8536 - accuracy: 0.7994 - val_loss: 0.9225 - val_accuracy: 0.7873\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8541 - accuracy: 0.8010 - val_loss: 0.9198 - val_accuracy: 0.7907\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8506 - accuracy: 0.8022 - val_loss: 0.9223 - val_accuracy: 0.7893\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8502 - accuracy: 0.8037 - val_loss: 0.9205 - val_accuracy: 0.7896\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8478 - accuracy: 0.8043 - val_loss: 0.9238 - val_accuracy: 0.7905\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8495 - accuracy: 0.8049 - val_loss: 0.9249 - val_accuracy: 0.7906\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8474 - accuracy: 0.8064 - val_loss: 0.9243 - val_accuracy: 0.7922\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8441 - accuracy: 0.8071 - val_loss: 0.9218 - val_accuracy: 0.7943\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8458 - accuracy: 0.8081 - val_loss: 0.9244 - val_accuracy: 0.7940\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8441 - accuracy: 0.8090 - val_loss: 0.9212 - val_accuracy: 0.7954\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8438 - accuracy: 0.8084 - val_loss: 0.9215 - val_accuracy: 0.7971\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8482 - accuracy: 0.8095 - val_loss: 0.9295 - val_accuracy: 0.7972\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8468 - accuracy: 0.8102 - val_loss: 0.9244 - val_accuracy: 0.7977\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8447 - accuracy: 0.8110 - val_loss: 0.9219 - val_accuracy: 0.7965\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8456 - accuracy: 0.8118 - val_loss: 0.9289 - val_accuracy: 0.7974\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8461 - accuracy: 0.8127 - val_loss: 0.9309 - val_accuracy: 0.7975\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8455 - accuracy: 0.8133 - val_loss: 0.9262 - val_accuracy: 0.8003\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8395 - accuracy: 0.8143 - val_loss: 0.9230 - val_accuracy: 0.8008\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8432 - accuracy: 0.8145 - val_loss: 0.9287 - val_accuracy: 0.8016\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8424 - accuracy: 0.8142 - val_loss: 0.9286 - val_accuracy: 0.8023\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8404 - accuracy: 0.8150 - val_loss: 0.9247 - val_accuracy: 0.8031\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.55 on slice CLASS=5\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.12 on slice CLASS=2\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 10000 training and 10000 test examples) achieved an AUC of 0.52\n",
      "  MULTI_LAYERED_PERCEPTRON (with 10000 training and 10000 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.06\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.07\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 8031 training and 8031 test examples) achieved an AUC of 0.51\n",
      "  MULTI_LAYERED_PERCEPTRON (with 8031 training and 8031 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  LOGISTIC_REGRESSION (with 1969 training and 1969 test examples) achieved an AUC of 0.52\n",
      "  LOGISTIC_REGRESSION (with 1969 training and 1969 test examples) achieved an advantage of 0.05\n",
      "========================================\n",
      "DP-SGD with sampling rate = 0.2% and noise_multiplier = 0.7533523513737009 iterated over 25000 steps satisfies differential privacy with eps = 4 and delta = 1e-06.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 2.3128 - accuracy: 0.3149 - val_loss: 1.4940 - val_accuracy: 0.5067\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 1.2565 - accuracy: 0.5904 - val_loss: 1.1411 - val_accuracy: 0.6376\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 1.0581 - accuracy: 0.6689 - val_loss: 1.0428 - val_accuracy: 0.6855\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.9856 - accuracy: 0.7038 - val_loss: 0.9962 - val_accuracy: 0.7103\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9426 - accuracy: 0.7255 - val_loss: 0.9774 - val_accuracy: 0.7250\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9211 - accuracy: 0.7393 - val_loss: 0.9559 - val_accuracy: 0.7348\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9010 - accuracy: 0.7492 - val_loss: 0.9441 - val_accuracy: 0.7407\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8888 - accuracy: 0.7582 - val_loss: 0.9342 - val_accuracy: 0.7483\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8820 - accuracy: 0.7645 - val_loss: 0.9281 - val_accuracy: 0.7554\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8746 - accuracy: 0.7697 - val_loss: 0.9264 - val_accuracy: 0.7632\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8660 - accuracy: 0.7747 - val_loss: 0.9193 - val_accuracy: 0.7649\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8577 - accuracy: 0.7778 - val_loss: 0.9114 - val_accuracy: 0.7711\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8512 - accuracy: 0.7821 - val_loss: 0.9037 - val_accuracy: 0.7731\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8506 - accuracy: 0.7853 - val_loss: 0.9053 - val_accuracy: 0.7750\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8473 - accuracy: 0.7878 - val_loss: 0.9034 - val_accuracy: 0.7769\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8442 - accuracy: 0.7913 - val_loss: 0.9038 - val_accuracy: 0.7803\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8405 - accuracy: 0.7926 - val_loss: 0.9008 - val_accuracy: 0.7823\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8362 - accuracy: 0.7948 - val_loss: 0.8967 - val_accuracy: 0.7825\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8349 - accuracy: 0.7977 - val_loss: 0.8988 - val_accuracy: 0.7852\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8300 - accuracy: 0.7986 - val_loss: 0.8959 - val_accuracy: 0.7859\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8302 - accuracy: 0.8013 - val_loss: 0.8948 - val_accuracy: 0.7909\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8304 - accuracy: 0.8017 - val_loss: 0.8992 - val_accuracy: 0.7942\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8318 - accuracy: 0.8030 - val_loss: 0.8982 - val_accuracy: 0.7954\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8294 - accuracy: 0.8040 - val_loss: 0.8978 - val_accuracy: 0.7962\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8270 - accuracy: 0.8056 - val_loss: 0.8976 - val_accuracy: 0.7969\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8280 - accuracy: 0.8061 - val_loss: 0.8985 - val_accuracy: 0.7971\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8288 - accuracy: 0.8068 - val_loss: 0.9006 - val_accuracy: 0.7987\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8287 - accuracy: 0.8086 - val_loss: 0.9021 - val_accuracy: 0.7979\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8282 - accuracy: 0.8091 - val_loss: 0.9034 - val_accuracy: 0.8001\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8279 - accuracy: 0.8109 - val_loss: 0.9019 - val_accuracy: 0.7987\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8276 - accuracy: 0.8123 - val_loss: 0.9039 - val_accuracy: 0.8002\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8276 - accuracy: 0.8126 - val_loss: 0.9055 - val_accuracy: 0.8014\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8272 - accuracy: 0.8140 - val_loss: 0.9054 - val_accuracy: 0.8023\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 31s 63ms/step - loss: 0.8262 - accuracy: 0.8147 - val_loss: 0.9083 - val_accuracy: 0.8018\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8229 - accuracy: 0.8156 - val_loss: 0.9058 - val_accuracy: 0.8030\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8239 - accuracy: 0.8165 - val_loss: 0.9147 - val_accuracy: 0.8013\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8247 - accuracy: 0.8171 - val_loss: 0.9103 - val_accuracy: 0.8044\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8247 - accuracy: 0.8175 - val_loss: 0.9108 - val_accuracy: 0.8045\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8249 - accuracy: 0.8185 - val_loss: 0.9178 - val_accuracy: 0.8035\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8218 - accuracy: 0.8195 - val_loss: 0.9094 - val_accuracy: 0.8034\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8151 - accuracy: 0.8203 - val_loss: 0.9017 - val_accuracy: 0.8032\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8145 - accuracy: 0.8211 - val_loss: 0.9016 - val_accuracy: 0.8051\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8122 - accuracy: 0.8212 - val_loss: 0.9034 - val_accuracy: 0.8064\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8101 - accuracy: 0.8217 - val_loss: 0.9046 - val_accuracy: 0.8067\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8107 - accuracy: 0.8224 - val_loss: 0.9027 - val_accuracy: 0.8077\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8115 - accuracy: 0.8227 - val_loss: 0.9041 - val_accuracy: 0.8084\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8124 - accuracy: 0.8234 - val_loss: 0.9047 - val_accuracy: 0.8093\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8132 - accuracy: 0.8229 - val_loss: 0.9050 - val_accuracy: 0.8077\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8125 - accuracy: 0.8238 - val_loss: 0.9095 - val_accuracy: 0.8065\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8124 - accuracy: 0.8248 - val_loss: 0.9033 - val_accuracy: 0.8086\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.57 on slice CLASS=7\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an advantage of 0.14 on slice CLASS=7\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 10000 training and 10000 test examples) achieved an AUC of 0.51\n",
      "  MULTI_LAYERED_PERCEPTRON (with 10000 training and 10000 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.13\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.56\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.13\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.06\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  THRESHOLD_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.14\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.57\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an advantage of 0.14\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  THRESHOLD_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.50\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 8086 training and 8086 test examples) achieved an AUC of 0.52\n",
      "  MULTI_LAYERED_PERCEPTRON (with 8086 training and 8086 test examples) achieved an advantage of 0.04\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  LOGISTIC_REGRESSION (with 1914 training and 1914 test examples) achieved an AUC of 0.53\n",
      "  LOGISTIC_REGRESSION (with 1914 training and 1914 test examples) achieved an advantage of 0.09\n",
      "========================================\n",
      "DP-SGD with sampling rate = 0.2% and noise_multiplier = 0.6070740997668997 iterated over 25000 steps satisfies differential privacy with eps = 8 and delta = 1e-06.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 1.8669 - accuracy: 0.4259 - val_loss: 1.2516 - val_accuracy: 0.5979\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 1.0959 - accuracy: 0.6548 - val_loss: 1.0387 - val_accuracy: 0.6877\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.9778 - accuracy: 0.7096 - val_loss: 0.9858 - val_accuracy: 0.7196\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.9349 - accuracy: 0.7336 - val_loss: 0.9462 - val_accuracy: 0.7370\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.9071 - accuracy: 0.7491 - val_loss: 0.9300 - val_accuracy: 0.7494\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.8888 - accuracy: 0.7601 - val_loss: 0.9207 - val_accuracy: 0.7605\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.8744 - accuracy: 0.7681 - val_loss: 0.9051 - val_accuracy: 0.7668\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.8625 - accuracy: 0.7750 - val_loss: 0.8964 - val_accuracy: 0.7704\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8559 - accuracy: 0.7806 - val_loss: 0.8992 - val_accuracy: 0.7760\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8528 - accuracy: 0.7838 - val_loss: 0.8910 - val_accuracy: 0.7796\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8427 - accuracy: 0.7890 - val_loss: 0.8787 - val_accuracy: 0.7841\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8385 - accuracy: 0.7921 - val_loss: 0.8813 - val_accuracy: 0.7863\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8373 - accuracy: 0.7952 - val_loss: 0.8783 - val_accuracy: 0.7893\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.8312 - accuracy: 0.7975 - val_loss: 0.8835 - val_accuracy: 0.7903\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.8297 - accuracy: 0.8000 - val_loss: 0.8828 - val_accuracy: 0.7930\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8262 - accuracy: 0.8024 - val_loss: 0.8733 - val_accuracy: 0.7959\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.8216 - accuracy: 0.8039 - val_loss: 0.8658 - val_accuracy: 0.7981\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.8149 - accuracy: 0.8065 - val_loss: 0.8646 - val_accuracy: 0.8002\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8148 - accuracy: 0.8079 - val_loss: 0.8695 - val_accuracy: 0.8002\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.8144 - accuracy: 0.8090 - val_loss: 0.8666 - val_accuracy: 0.8031\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8132 - accuracy: 0.8106 - val_loss: 0.8689 - val_accuracy: 0.8055\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.8117 - accuracy: 0.8115 - val_loss: 0.8728 - val_accuracy: 0.8039\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8093 - accuracy: 0.8131 - val_loss: 0.8718 - val_accuracy: 0.8062\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8072 - accuracy: 0.8147 - val_loss: 0.8767 - val_accuracy: 0.8060\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8062 - accuracy: 0.8150 - val_loss: 0.8766 - val_accuracy: 0.8076\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.8073 - accuracy: 0.8163 - val_loss: 0.8742 - val_accuracy: 0.8093\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8068 - accuracy: 0.8172 - val_loss: 0.8762 - val_accuracy: 0.8096\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.8058 - accuracy: 0.8191 - val_loss: 0.8788 - val_accuracy: 0.8093\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.8027 - accuracy: 0.8191 - val_loss: 0.8748 - val_accuracy: 0.8100\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.8010 - accuracy: 0.8206 - val_loss: 0.8822 - val_accuracy: 0.8100\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.8004 - accuracy: 0.8213 - val_loss: 0.8806 - val_accuracy: 0.8118\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 34s 69ms/step - loss: 0.8008 - accuracy: 0.8226 - val_loss: 0.8826 - val_accuracy: 0.8101\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.7997 - accuracy: 0.8232 - val_loss: 0.8824 - val_accuracy: 0.8129\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.7985 - accuracy: 0.8243 - val_loss: 0.8853 - val_accuracy: 0.8124\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.7968 - accuracy: 0.8244 - val_loss: 0.8834 - val_accuracy: 0.8129\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.7962 - accuracy: 0.8253 - val_loss: 0.8798 - val_accuracy: 0.8118\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.7971 - accuracy: 0.8264 - val_loss: 0.8881 - val_accuracy: 0.8136\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.7970 - accuracy: 0.8263 - val_loss: 0.8893 - val_accuracy: 0.8154\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.7985 - accuracy: 0.8273 - val_loss: 0.8849 - val_accuracy: 0.8158\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.7965 - accuracy: 0.8275 - val_loss: 0.8901 - val_accuracy: 0.8161\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.7968 - accuracy: 0.8280 - val_loss: 0.8854 - val_accuracy: 0.8151\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.7938 - accuracy: 0.8281 - val_loss: 0.8829 - val_accuracy: 0.8170\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.7928 - accuracy: 0.8284 - val_loss: 0.8855 - val_accuracy: 0.8171\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.7931 - accuracy: 0.8291 - val_loss: 0.8861 - val_accuracy: 0.8184\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.7946 - accuracy: 0.8301 - val_loss: 0.8929 - val_accuracy: 0.8181\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 35s 70ms/step - loss: 0.7940 - accuracy: 0.8299 - val_loss: 0.8923 - val_accuracy: 0.8192\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 34s 69ms/step - loss: 0.7945 - accuracy: 0.8301 - val_loss: 0.8946 - val_accuracy: 0.8178\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.7938 - accuracy: 0.8314 - val_loss: 0.8931 - val_accuracy: 0.8174\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.7940 - accuracy: 0.8324 - val_loss: 0.8950 - val_accuracy: 0.8174\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 35s 69ms/step - loss: 0.7939 - accuracy: 0.8328 - val_loss: 0.8948 - val_accuracy: 0.8169\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.56 on slice CLASS=4\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.13 on slice CLASS=6\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  THRESHOLD_ATTACK (with 50000 training and 10000 test examples) achieved an AUC of 0.51\n",
      "  LOGISTIC_REGRESSION (with 10000 training and 10000 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.56\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.13\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  THRESHOLD_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.50\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  RANDOM_FOREST (with 8169 training and 8169 test examples) achieved an AUC of 0.51\n",
      "  RANDOM_FOREST (with 8169 training and 8169 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  RANDOM_FOREST (with 1831 training and 1831 test examples) achieved an AUC of 0.53\n",
      "  K_NEAREST_NEIGHBORS (with 1831 training and 1831 test examples) achieved an advantage of 0.08\n",
      "========================================\n",
      "DP-SGD with sampling rate = 0.2% and noise_multiplier = 0.5065254375835421 iterated over 25000 steps satisfies differential privacy with eps = 16 and delta = 1e-06.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 34s 67ms/step - loss: 1.8849 - accuracy: 0.4191 - val_loss: 1.2230 - val_accuracy: 0.6069\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 1.0842 - accuracy: 0.6627 - val_loss: 1.0314 - val_accuracy: 0.6938\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.9716 - accuracy: 0.7161 - val_loss: 0.9747 - val_accuracy: 0.7218\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.9223 - accuracy: 0.7414 - val_loss: 0.9360 - val_accuracy: 0.7413\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8971 - accuracy: 0.7558 - val_loss: 0.9204 - val_accuracy: 0.7540\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.8809 - accuracy: 0.7664 - val_loss: 0.9116 - val_accuracy: 0.7619\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8676 - accuracy: 0.7743 - val_loss: 0.9020 - val_accuracy: 0.7686\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.8569 - accuracy: 0.7798 - val_loss: 0.8960 - val_accuracy: 0.7736\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8478 - accuracy: 0.7859 - val_loss: 0.8915 - val_accuracy: 0.7776\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8398 - accuracy: 0.7895 - val_loss: 0.8900 - val_accuracy: 0.7818\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8364 - accuracy: 0.7936 - val_loss: 0.8899 - val_accuracy: 0.7860\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8316 - accuracy: 0.7969 - val_loss: 0.8856 - val_accuracy: 0.7871\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8268 - accuracy: 0.7998 - val_loss: 0.8806 - val_accuracy: 0.7919\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.8244 - accuracy: 0.8027 - val_loss: 0.8824 - val_accuracy: 0.7888\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.8192 - accuracy: 0.8045 - val_loss: 0.8714 - val_accuracy: 0.7936\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8165 - accuracy: 0.8060 - val_loss: 0.8768 - val_accuracy: 0.7963\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 34s 67ms/step - loss: 0.8146 - accuracy: 0.8076 - val_loss: 0.8779 - val_accuracy: 0.7961\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8098 - accuracy: 0.8102 - val_loss: 0.8697 - val_accuracy: 0.8000\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8063 - accuracy: 0.8120 - val_loss: 0.8715 - val_accuracy: 0.8012\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 34s 67ms/step - loss: 0.8048 - accuracy: 0.8123 - val_loss: 0.8766 - val_accuracy: 0.8001\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8028 - accuracy: 0.8144 - val_loss: 0.8706 - val_accuracy: 0.8026\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8018 - accuracy: 0.8158 - val_loss: 0.8757 - val_accuracy: 0.8024\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7991 - accuracy: 0.8166 - val_loss: 0.8665 - val_accuracy: 0.8052\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.7945 - accuracy: 0.8188 - val_loss: 0.8736 - val_accuracy: 0.8064\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7950 - accuracy: 0.8194 - val_loss: 0.8693 - val_accuracy: 0.8072\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7942 - accuracy: 0.8203 - val_loss: 0.8729 - val_accuracy: 0.8081\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7933 - accuracy: 0.8220 - val_loss: 0.8741 - val_accuracy: 0.8100\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7915 - accuracy: 0.8229 - val_loss: 0.8763 - val_accuracy: 0.8099\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7909 - accuracy: 0.8235 - val_loss: 0.8762 - val_accuracy: 0.8122\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7872 - accuracy: 0.8242 - val_loss: 0.8730 - val_accuracy: 0.8141\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7856 - accuracy: 0.8250 - val_loss: 0.8745 - val_accuracy: 0.8136\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7856 - accuracy: 0.8264 - val_loss: 0.8719 - val_accuracy: 0.8129\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7816 - accuracy: 0.8271 - val_loss: 0.8739 - val_accuracy: 0.8139\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 34s 67ms/step - loss: 0.7812 - accuracy: 0.8278 - val_loss: 0.8743 - val_accuracy: 0.8145\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7800 - accuracy: 0.8284 - val_loss: 0.8719 - val_accuracy: 0.8150\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7780 - accuracy: 0.8294 - val_loss: 0.8743 - val_accuracy: 0.8154\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7768 - accuracy: 0.8299 - val_loss: 0.8797 - val_accuracy: 0.8129\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 34s 67ms/step - loss: 0.7776 - accuracy: 0.8309 - val_loss: 0.8722 - val_accuracy: 0.8137\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7753 - accuracy: 0.8321 - val_loss: 0.8727 - val_accuracy: 0.8152\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7747 - accuracy: 0.8321 - val_loss: 0.8736 - val_accuracy: 0.8166\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.7728 - accuracy: 0.8333 - val_loss: 0.8748 - val_accuracy: 0.8172\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.7704 - accuracy: 0.8337 - val_loss: 0.8738 - val_accuracy: 0.8192\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7694 - accuracy: 0.8344 - val_loss: 0.8744 - val_accuracy: 0.8167\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7686 - accuracy: 0.8349 - val_loss: 0.8782 - val_accuracy: 0.8185\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.7702 - accuracy: 0.8355 - val_loss: 0.8753 - val_accuracy: 0.8182\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7704 - accuracy: 0.8357 - val_loss: 0.8721 - val_accuracy: 0.8189\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7688 - accuracy: 0.8364 - val_loss: 0.8756 - val_accuracy: 0.8188\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.7683 - accuracy: 0.8367 - val_loss: 0.8737 - val_accuracy: 0.8185\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 33s 67ms/step - loss: 0.7680 - accuracy: 0.8381 - val_loss: 0.8733 - val_accuracy: 0.8203\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7668 - accuracy: 0.8386 - val_loss: 0.8751 - val_accuracy: 0.8181\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.56 on slice CLASS=2\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.14 on slice CLASS=7\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  RANDOM_FOREST (with 10000 training and 10000 test examples) achieved an AUC of 0.52\n",
      "  RANDOM_FOREST (with 10000 training and 10000 test examples) achieved an advantage of 0.04\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.56\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.56\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  THRESHOLD_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  THRESHOLD_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.50\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.14\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  THRESHOLD_ENTROPY_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.50\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  LOGISTIC_REGRESSION (with 8181 training and 8181 test examples) achieved an AUC of 0.51\n",
      "  MULTI_LAYERED_PERCEPTRON (with 8181 training and 8181 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  LOGISTIC_REGRESSION (with 1819 training and 1819 test examples) achieved an AUC of 0.52\n",
      "  LOGISTIC_REGRESSION (with 1819 training and 1819 test examples) achieved an advantage of 0.05\n",
      "========================================\n",
      "DP-SGD with sampling rate = 0.2% and noise_multiplier = 0.3277833301558402 iterated over 25000 steps satisfies differential privacy with eps = 100 and delta = 1e-06.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.7500 - accuracy: 0.4698 - val_loss: 1.0916 - val_accuracy: 0.6596\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9853 - accuracy: 0.7021 - val_loss: 0.9733 - val_accuracy: 0.7227\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.9042 - accuracy: 0.7451 - val_loss: 0.9226 - val_accuracy: 0.7474\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8666 - accuracy: 0.7649 - val_loss: 0.9077 - val_accuracy: 0.7618\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8480 - accuracy: 0.7764 - val_loss: 0.9015 - val_accuracy: 0.7710\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8393 - accuracy: 0.7838 - val_loss: 0.8824 - val_accuracy: 0.7802\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8312 - accuracy: 0.7915 - val_loss: 0.8923 - val_accuracy: 0.7839\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8249 - accuracy: 0.7976 - val_loss: 0.8746 - val_accuracy: 0.7923\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8157 - accuracy: 0.8014 - val_loss: 0.8794 - val_accuracy: 0.7923\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8051 - accuracy: 0.8054 - val_loss: 0.8679 - val_accuracy: 0.7972\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8003 - accuracy: 0.8081 - val_loss: 0.8713 - val_accuracy: 0.8022\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7999 - accuracy: 0.8113 - val_loss: 0.8680 - val_accuracy: 0.8026\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7965 - accuracy: 0.8142 - val_loss: 0.8686 - val_accuracy: 0.8040\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7913 - accuracy: 0.8166 - val_loss: 0.8709 - val_accuracy: 0.8044\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 0.7884 - accuracy: 0.8188 - val_loss: 0.8679 - val_accuracy: 0.8069\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7859 - accuracy: 0.8199 - val_loss: 0.8716 - val_accuracy: 0.8092\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7807 - accuracy: 0.8219 - val_loss: 0.8728 - val_accuracy: 0.8084\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 31s 63ms/step - loss: 0.7774 - accuracy: 0.8230 - val_loss: 0.8652 - val_accuracy: 0.8117\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7732 - accuracy: 0.8247 - val_loss: 0.8687 - val_accuracy: 0.8127\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7736 - accuracy: 0.8253 - val_loss: 0.8697 - val_accuracy: 0.8130\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7728 - accuracy: 0.8267 - val_loss: 0.8744 - val_accuracy: 0.8117\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7670 - accuracy: 0.8278 - val_loss: 0.8656 - val_accuracy: 0.8135\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7634 - accuracy: 0.8293 - val_loss: 0.8619 - val_accuracy: 0.8143\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7628 - accuracy: 0.8304 - val_loss: 0.8588 - val_accuracy: 0.8167\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7603 - accuracy: 0.8318 - val_loss: 0.8690 - val_accuracy: 0.8170\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7565 - accuracy: 0.8329 - val_loss: 0.8720 - val_accuracy: 0.8151\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7559 - accuracy: 0.8345 - val_loss: 0.8664 - val_accuracy: 0.8165\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7552 - accuracy: 0.8354 - val_loss: 0.8611 - val_accuracy: 0.8191\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7529 - accuracy: 0.8354 - val_loss: 0.8675 - val_accuracy: 0.8191\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7526 - accuracy: 0.8376 - val_loss: 0.8636 - val_accuracy: 0.8206\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7519 - accuracy: 0.8370 - val_loss: 0.8702 - val_accuracy: 0.8211\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7509 - accuracy: 0.8382 - val_loss: 0.8693 - val_accuracy: 0.8208\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7513 - accuracy: 0.8390 - val_loss: 0.8640 - val_accuracy: 0.8216\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7491 - accuracy: 0.8399 - val_loss: 0.8655 - val_accuracy: 0.8214\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7440 - accuracy: 0.8413 - val_loss: 0.8610 - val_accuracy: 0.8224\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7431 - accuracy: 0.8418 - val_loss: 0.8689 - val_accuracy: 0.8235\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7409 - accuracy: 0.8426 - val_loss: 0.8647 - val_accuracy: 0.8226\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7401 - accuracy: 0.8433 - val_loss: 0.8676 - val_accuracy: 0.8249\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7380 - accuracy: 0.8440 - val_loss: 0.8786 - val_accuracy: 0.8250\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7373 - accuracy: 0.8447 - val_loss: 0.8682 - val_accuracy: 0.8253\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7348 - accuracy: 0.8455 - val_loss: 0.8678 - val_accuracy: 0.8254\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7333 - accuracy: 0.8464 - val_loss: 0.8807 - val_accuracy: 0.8255\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7352 - accuracy: 0.8468 - val_loss: 0.8742 - val_accuracy: 0.8260\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7342 - accuracy: 0.8466 - val_loss: 0.8756 - val_accuracy: 0.8275\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 0.7324 - accuracy: 0.8473 - val_loss: 0.8715 - val_accuracy: 0.8277\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7331 - accuracy: 0.8480 - val_loss: 0.8739 - val_accuracy: 0.8285\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7313 - accuracy: 0.8491 - val_loss: 0.8767 - val_accuracy: 0.8265\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7300 - accuracy: 0.8484 - val_loss: 0.8765 - val_accuracy: 0.8284\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7309 - accuracy: 0.8492 - val_loss: 0.8779 - val_accuracy: 0.8284\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7298 - accuracy: 0.8494 - val_loss: 0.8746 - val_accuracy: 0.8280\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.58 on slice CLASS=5\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.14 on slice CLASS=7\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  THRESHOLD_ATTACK (with 50000 training and 10000 test examples) achieved an AUC of 0.51\n",
      "  THRESHOLD_ENTROPY_ATTACK (with 50000 training and 10000 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  THRESHOLD_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.13\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.58\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.13\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.13\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.14\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.52\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.08\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.56\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  THRESHOLD_ATTACK (with 42503 training and 8280 test examples) achieved an AUC of 0.50\n",
      "  LOGISTIC_REGRESSION (with 8280 training and 8280 test examples) achieved an advantage of 0.03\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  K_NEAREST_NEIGHBORS (with 1720 training and 1720 test examples) achieved an AUC of 0.55\n",
      "  RANDOM_FOREST (with 1720 training and 1720 test examples) achieved an advantage of 0.09\n",
      "========================================\n",
      "DP-SGD with sampling rate = 0.2% and noise_multiplier = 0.21698811977843638 iterated over 25000 steps satisfies differential privacy with eps = 1e+03 and delta = 1e-06.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 1.4505 - accuracy: 0.5566 - val_loss: 0.9947 - val_accuracy: 0.7033\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 0.9043 - accuracy: 0.7386 - val_loss: 0.9033 - val_accuracy: 0.7499\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.8442 - accuracy: 0.7693 - val_loss: 0.8829 - val_accuracy: 0.7694\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.8208 - accuracy: 0.7862 - val_loss: 0.8691 - val_accuracy: 0.7790\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 33s 66ms/step - loss: 0.8060 - accuracy: 0.7964 - val_loss: 0.8549 - val_accuracy: 0.7885\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7911 - accuracy: 0.8025 - val_loss: 0.8567 - val_accuracy: 0.7927\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7842 - accuracy: 0.8088 - val_loss: 0.8624 - val_accuracy: 0.7966\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 0.7757 - accuracy: 0.8131 - val_loss: 0.8519 - val_accuracy: 0.8009\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7707 - accuracy: 0.8173 - val_loss: 0.8632 - val_accuracy: 0.8019\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7656 - accuracy: 0.8205 - val_loss: 0.8475 - val_accuracy: 0.8067\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7610 - accuracy: 0.8231 - val_loss: 0.8499 - val_accuracy: 0.8060\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 32s 63ms/step - loss: 0.7555 - accuracy: 0.8253 - val_loss: 0.8436 - val_accuracy: 0.8105\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7506 - accuracy: 0.8280 - val_loss: 0.8370 - val_accuracy: 0.8143\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7480 - accuracy: 0.8293 - val_loss: 0.8479 - val_accuracy: 0.8151\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7451 - accuracy: 0.8318 - val_loss: 0.8480 - val_accuracy: 0.8181\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7441 - accuracy: 0.8334 - val_loss: 0.8472 - val_accuracy: 0.8165\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7395 - accuracy: 0.8344 - val_loss: 0.8541 - val_accuracy: 0.8183\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7383 - accuracy: 0.8357 - val_loss: 0.8486 - val_accuracy: 0.8223\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7351 - accuracy: 0.8379 - val_loss: 0.8474 - val_accuracy: 0.8222\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7321 - accuracy: 0.8392 - val_loss: 0.8464 - val_accuracy: 0.8246\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7309 - accuracy: 0.8400 - val_loss: 0.8480 - val_accuracy: 0.8232\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7299 - accuracy: 0.8413 - val_loss: 0.8435 - val_accuracy: 0.8265\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7258 - accuracy: 0.8428 - val_loss: 0.8503 - val_accuracy: 0.8264\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7293 - accuracy: 0.8428 - val_loss: 0.8497 - val_accuracy: 0.8288\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7237 - accuracy: 0.8442 - val_loss: 0.8509 - val_accuracy: 0.8282\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7204 - accuracy: 0.8455 - val_loss: 0.8533 - val_accuracy: 0.8270\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7204 - accuracy: 0.8459 - val_loss: 0.8485 - val_accuracy: 0.8290\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7195 - accuracy: 0.8477 - val_loss: 0.8511 - val_accuracy: 0.8288\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7159 - accuracy: 0.8481 - val_loss: 0.8532 - val_accuracy: 0.8306\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7130 - accuracy: 0.8489 - val_loss: 0.8506 - val_accuracy: 0.8278\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7126 - accuracy: 0.8499 - val_loss: 0.8517 - val_accuracy: 0.8282\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7102 - accuracy: 0.8506 - val_loss: 0.8662 - val_accuracy: 0.8277\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7116 - accuracy: 0.8504 - val_loss: 0.8615 - val_accuracy: 0.8300\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7070 - accuracy: 0.8517 - val_loss: 0.8599 - val_accuracy: 0.8298\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.7043 - accuracy: 0.8522 - val_loss: 0.8558 - val_accuracy: 0.8309\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7045 - accuracy: 0.8541 - val_loss: 0.8613 - val_accuracy: 0.8316\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.7015 - accuracy: 0.8540 - val_loss: 0.8575 - val_accuracy: 0.8306\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.7003 - accuracy: 0.8543 - val_loss: 0.8640 - val_accuracy: 0.8329\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.6994 - accuracy: 0.8539 - val_loss: 0.8698 - val_accuracy: 0.8323\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.6988 - accuracy: 0.8550 - val_loss: 0.8640 - val_accuracy: 0.8333\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.6983 - accuracy: 0.8554 - val_loss: 0.8715 - val_accuracy: 0.8329\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.6996 - accuracy: 0.8568 - val_loss: 0.8692 - val_accuracy: 0.8359\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.6985 - accuracy: 0.8567 - val_loss: 0.8687 - val_accuracy: 0.8362\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.6973 - accuracy: 0.8571 - val_loss: 0.8710 - val_accuracy: 0.8354\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.6947 - accuracy: 0.8580 - val_loss: 0.8748 - val_accuracy: 0.8361\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.6944 - accuracy: 0.8584 - val_loss: 0.8773 - val_accuracy: 0.8361\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.6944 - accuracy: 0.8595 - val_loss: 0.8748 - val_accuracy: 0.8357\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 32s 64ms/step - loss: 0.6926 - accuracy: 0.8600 - val_loss: 0.8754 - val_accuracy: 0.8360\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 32s 65ms/step - loss: 0.6898 - accuracy: 0.8605 - val_loss: 0.8732 - val_accuracy: 0.8362\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 33s 65ms/step - loss: 0.6912 - accuracy: 0.8616 - val_loss: 0.8766 - val_accuracy: 0.8377\n",
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.55 on slice CLASS=9\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.13 on slice CLASS=6\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  RANDOM_FOREST (with 10000 training and 10000 test examples) achieved an AUC of 0.52\n",
      "  RANDOM_FOREST (with 10000 training and 10000 test examples) achieved an advantage of 0.04\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  THRESHOLD_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.54\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.11\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.09\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  MULTI_LAYERED_PERCEPTRON (with 1000 training and 1000 test examples) achieved an advantage of 0.13\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an AUC of 0.53\n",
      "  RANDOM_FOREST (with 1000 training and 1000 test examples) achieved an advantage of 0.12\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  THRESHOLD_ENTROPY_ATTACK (with 5000 training and 1000 test examples) achieved an AUC of 0.51\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  K_NEAREST_NEIGHBORS (with 1000 training and 1000 test examples) achieved an AUC of 0.55\n",
      "  LOGISTIC_REGRESSION (with 1000 training and 1000 test examples) achieved an advantage of 0.10\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  RANDOM_FOREST (with 8377 training and 8377 test examples) achieved an AUC of 0.51\n",
      "  RANDOM_FOREST (with 8377 training and 8377 test examples) achieved an advantage of 0.02\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  THRESHOLD_ENTROPY_ATTACK (with 6882 training and 1623 test examples) achieved an AUC of 0.53\n",
      "  LOGISTIC_REGRESSION (with 1623 training and 1623 test examples) achieved an advantage of 0.08\n",
      "========================================\n",
      "Epochs, Target epsilon, delta, C, Sigma, Sampling rate, Epsilon, Loss, Val loss, Accuracy, Val accuracy, Time, AUC, Advantage\n",
      "50, 0.1, 1e-06, 1, 13.133789235896893, 0.002, 0.11208687095631774, 1.2391196489334106, 1.2499029636383057, 0.6535400152206421, 0.6535999774932861, 1605.6922137737274, 0.5645439999999999, 0.11200000000000004\n",
      "50, 0.5, 1e-06, 1, 2.8456245157566924, 0.002, 0.49999999999976064, 0.9386911988258362, 1.0134037733078003, 0.7723199725151062, 0.7577999830245972, 1620.4611415863037, 0.55192, 0.12400000000000005\n",
      "50, 1, 1e-06, 1, 1.6022653845391852, 0.002, 0.9999999999992891, 0.8718630075454712, 0.9253203272819519, 0.7979400157928467, 0.791100025177002, 1627.3003470897675, 0.5623039999999999, 0.16000000000000003\n",
      "50, 2, 1e-06, 1, 1.013450319287938, 0.002, 2.0000000000038187, 0.8404057025909424, 0.9247042536735535, 0.8149999976158142, 0.8030999898910522, 1618.0348279476166, 0.548368, 0.12399999999999994\n",
      "50, 4, 1e-06, 1, 0.7533523513737009, 0.002, 3.9718294983120996, 0.8124180436134338, 0.9033370018005371, 0.8247600197792053, 0.8086000084877014, 1615.052188873291, 0.5678, 0.14\n",
      "50, 8, 1e-06, 1, 0.6070740997668997, 0.002, 7.995640021330993, 0.7939220666885376, 0.8948453068733215, 0.8327999711036682, 0.8169000148773193, 1728.3421576023102, 0.5557599999999999, 0.128\n",
      "50, 16, 1e-06, 1, 0.5065254375835421, 0.002, 15.936945091865898, 0.7668374180793762, 0.8751069903373718, 0.8385800123214722, 0.8180999755859375, 1652.9493680000305, 0.5634079999999999, 0.14399999999999996\n",
      "50, 100, 1e-06, 1, 0.3277833301558402, 0.002, 97.52025332082994, 0.7298278212547302, 0.8745848536491394, 0.8493599891662598, 0.828000009059906, 1612.918312072754, 0.5804480000000001, 0.136\n",
      "50, 1000, 1e-06, 1, 0.21698811977843638, 0.002, 483.49732699060695, 0.691163957118988, 0.8765596747398376, 0.8615800142288208, 0.8377000093460083, 1614.0465104579926, 0.554912, 0.128\n"
     ]
    }
   ],
   "source": [
    "# TEST FOR DIFFERENT EPS\n",
    "results_summary = []\n",
    "\n",
    "n = X_train.shape[0]\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "microbatches = 100\n",
    "epsilons = [0.1,0.5,1,2,4,8,16,100,1000]\n",
    "delta = 1e-6\n",
    "min_noise = 1e-100\n",
    "l2_norm_clip = 1\n",
    "sampling_rate = batch_size / n\n",
    "attacks = 1\n",
    "\n",
    "for e in epsilons:\n",
    "    # Compute noise multiplier from target epsilon\n",
    "    noise_multiplier = compute_noise(n, batch_size, e, epochs, delta, min_noise)\n",
    "    \n",
    "    # Compute epsilon\n",
    "    orders = [1 + x / 10. for x in range(1, 100)] + list(range(11, 101))\n",
    "    sampling_probability = batch_size / n\n",
    "    rdp = compute_rdp(q=sampling_probability,\n",
    "                    noise_multiplier=noise_multiplier,\n",
    "                    steps=epochs * n // batch_size,\n",
    "                    orders=orders)\n",
    "    eps = get_privacy_spent(orders, rdp, target_delta=delta)\n",
    "\n",
    "    # Instantiate network\n",
    "    classifierDP = create_networkDP(noise_multiplier, l2_norm_clip, microbatches)\n",
    "    \n",
    "    # Train network until convergence    \n",
    "    start_time = time.time()\n",
    "    r = classifierDP.fit(features_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(features_valid, y_test),\n",
    "                    shuffle=True,\n",
    "                    #callbacks=[lr_reduce],\n",
    "                    verbose=1\n",
    "                   )\n",
    "    end_time = time.time()\n",
    "\n",
    "    modelDP = Sequential([feature_extractor, classifierDP])\n",
    "    modelDP.compile(loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
    "    time_elapsed = (end_time - start_time)\n",
    "\n",
    "    # MIA \n",
    "    aauc = []\n",
    "    aadv = []\n",
    "    for _ in range(attacks):\n",
    "        auc, adv = membership_inference_attack(modelDP, X_train, X_test, y_train, y_test)\n",
    "        aauc.append(auc)\n",
    "        aadv.append(adv)\n",
    "    mauc = sum(aauc) / attacks\n",
    "    madv = sum(aadv) / attacks\n",
    "\n",
    "    # Write result summary\n",
    "    summ = ', '.join(map(str,[\n",
    "          len(r.history['loss']),\n",
    "          e,\n",
    "          delta,\n",
    "          l2_norm_clip,\n",
    "          noise_multiplier,\n",
    "          sampling_rate,\n",
    "          eps[0],\n",
    "          r.history['loss'][-1], \n",
    "          r.history['val_loss'][-1],\n",
    "          r.history['accuracy'][-1],\n",
    "          r.history['val_accuracy'][-1],\n",
    "          time_elapsed,\n",
    "          mauc,\n",
    "          madv\n",
    "    ]))\n",
    "    results_summary.append(summ)\n",
    "    print('='*40)\n",
    "\n",
    "print('Epochs, Target epsilon, delta, C, Sigma, Sampling rate, Epsilon, Loss, Val loss, Accuracy, Val accuracy, Time, AUC, Advantage')    \n",
    "for r in results_summary:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
