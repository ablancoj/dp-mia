{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, MaxPooling2D, BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer\n",
    "from tensorflow_privacy.privacy.analysis.compute_noise_from_budget_lib import compute_noise\n",
    "\n",
    "\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import membership_inference_attack as mia\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackInputData\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import SlicingSpec\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackType\n",
    "\n",
    "import tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.plotting as plotting\n",
    "\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d5766",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 15e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7336b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "    train_data, train_labels = train\n",
    "    test_data, test_labels = test\n",
    "\n",
    "    train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "    test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "    train_data = train_data.reshape((train_data.shape[0], 28, 28, 1))\n",
    "    test_data = test_data.reshape((test_data.shape[0], 28, 28, 1))\n",
    "\n",
    "    train_labels = np.array(train_labels, dtype=np.int32)\n",
    "    test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "    test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "    assert train_data.min() == 0.\n",
    "    assert train_data.max() == 1.\n",
    "    assert test_data.min() == 0.\n",
    "    assert test_data.max() == 1.\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def membership_inference_attack(model, X_train, X_test, y_train, y_test):\n",
    "    print('Predict on train...')\n",
    "    logits_train = model.predict(X_train, batch_size=batch_size)\n",
    "    print('Predict on test...')\n",
    "    logits_test = model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "    print('Apply softmax to get probabilities from logits...')\n",
    "    prob_train = special.softmax(logits_train, axis=1)\n",
    "    prob_test = special.softmax(logits_test, axis=1)\n",
    "\n",
    "    print('Compute losses...')\n",
    "    cce = tf.keras.backend.categorical_crossentropy\n",
    "    constant = tf.keras.backend.constant\n",
    "\n",
    "    loss_train = cce(constant(y_train), constant(prob_train), from_logits=False).numpy()\n",
    "    loss_test = cce(constant(y_test), constant(prob_test), from_logits=False).numpy()\n",
    "    \n",
    "    labels_train = np.argmax(y_train, axis=1)\n",
    "    labels_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    input = AttackInputData(\n",
    "      logits_train = logits_train,\n",
    "      logits_test = logits_test,\n",
    "      loss_train = loss_train,\n",
    "      loss_test = loss_test,\n",
    "      labels_train = labels_train,\n",
    "      labels_test = labels_test\n",
    "    )\n",
    "\n",
    "    # Run several attacks for different data slices\n",
    "    attacks_result = mia.run_attacks(input,\n",
    "                                     SlicingSpec(\n",
    "                                         entire_dataset = True,\n",
    "                                         by_class = True,\n",
    "                                         by_classification_correctness = True\n",
    "                                     ),\n",
    "                                     attack_types = [\n",
    "                                         AttackType.THRESHOLD_ATTACK,\n",
    "                                         AttackType.LOGISTIC_REGRESSION,\n",
    "                                         AttackType.MULTI_LAYERED_PERCEPTRON,\n",
    "                                         AttackType.RANDOM_FOREST, \n",
    "                                         AttackType.K_NEAREST_NEIGHBORS,\n",
    "                                         AttackType.THRESHOLD_ENTROPY_ATTACK\n",
    "                                     ])\n",
    "\n",
    "    # Plot the ROC curve of the best classifier\n",
    "#     fig = plotting.plot_roc_curve(\n",
    "#         attacks_result.get_result_with_max_auc().roc_curve)\n",
    "\n",
    "    # Print a user-friendly summary of the attacks\n",
    "    print(attacks_result.summary(by_slices = True))\n",
    "    time.sleep(5)\n",
    "    return attacks_result.get_result_with_max_auc().get_auc(), attacks_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(dropout=None, regularizer=None):\n",
    "    input_data = Input(shape = X_train[0].shape)\n",
    "    x = Reshape(target_shape=(28, 28, 1))(input_data)\n",
    "    x = Conv2D(filters=12, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(x)\n",
    "    output = Dense(10, kernel_regularizer=regularizer)(x)\n",
    "\n",
    "    model = Model(input_data, output)\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "            \n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_dp_cnn(noise_multiplier, l2_norm_clip, microbatches):\n",
    "    input_data = Input(shape = X_train[0].shape)\n",
    "    x = Reshape(target_shape=(28, 28, 1))(input_data)\n",
    "    x = Conv2D(filters=12, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    output = Dense(10)(x)\n",
    "\n",
    "    model = Model(input_data, output)\n",
    "    \n",
    "    optimizer = DPKerasAdamOptimizer(\n",
    "                            l2_norm_clip=l2_norm_clip,\n",
    "                            noise_multiplier=noise_multiplier,\n",
    "                            num_microbatches=microbatches,\n",
    "                            learning_rate=learning_rate)\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "X_train_flat, X_test_flat = X_train.reshape(-1, 28*28*1), X_test.reshape(-1, 28*28*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b5777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 100\n",
    "attacks = 5\n",
    "settings = [\n",
    "    (None,None),\n",
    "    (0.25,None),\n",
    "    (0.50,None),\n",
    "    (0.75,None),\n",
    "    (None,'l2'),\n",
    "    (0.25,'l2'),\n",
    "    (0.50,'l2'),\n",
    "    (0.75,'l2'),\n",
    "]\n",
    "results_summary = []\n",
    "\n",
    "for drop, reg in settings:\n",
    "    # Instantiate network\n",
    "    model = create_cnn(dropout=drop, regularizer=reg)\n",
    "    \n",
    "    # Train network until convergence\n",
    "    start_time = time.time()\n",
    "    r = model.fit(X_train, \n",
    "                y_train, \n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=epochs, \n",
    "                batch_size=batch_size,\n",
    "                #callbacks=[callback]\n",
    "               )\n",
    "    end_time = time.time()\n",
    "    time_elapsed = (end_time - start_time)\n",
    "\n",
    "    # MIA \n",
    "    aauc = []\n",
    "    aadv = []\n",
    "    for _ in range(attacks):\n",
    "        auc, adv = membership_inference_attack(model, X_train, X_test, y_train, y_test)\n",
    "        aauc.append(auc)\n",
    "        aadv.append(adv)\n",
    "    mauc = sum(aauc) / attacks\n",
    "    madv = sum(aadv) / attacks\n",
    "\n",
    "    # Write result summary\n",
    "    summ = ', '.join(map(str,[\n",
    "        len(r.history['loss']), #epochs\n",
    "        drop,\n",
    "        reg,\n",
    "        r.history['loss'][-1], \n",
    "        r.history['val_loss'][-1],\n",
    "        r.history['accuracy'][-1],\n",
    "        r.history['val_accuracy'][-1],\n",
    "        time_elapsed,\n",
    "        mauc,\n",
    "        madv\n",
    "    ]))\n",
    "\n",
    "    results_summary.append(summ)\n",
    "    print('='*40)\n",
    "    \n",
    "print('Epochs, Dropout, Regularizer, Loss, Val loss, Accuracy, Val accuracy, Time, AUC, Advantage')\n",
    "for r in results_summary:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f750ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST FOR DIFFERENT EPS\n",
    "results_summary = []\n",
    "\n",
    "n = X_train.shape[0]\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "microbatches = 100\n",
    "#epsilons = [0.1,0.5,1,2,4,8,16,100,1000]\n",
    "epsilons = np.linspace(0.1, np.log(2), 5) # 5 values from 0.1 to ln2\n",
    "#delta = 1e-6\n",
    "delta = 1e-100 # very small delta\n",
    "min_noise = 1e-100\n",
    "l2_norm_clip = 2.5\n",
    "sampling_rate = batch_size / n\n",
    "attacks = 5\n",
    "\n",
    "for e in epsilons:\n",
    "    # Compute noise multiplier from target epsilon\n",
    "    noise_multiplier = compute_noise(n, batch_size, e, epochs, delta, min_noise)\n",
    "    \n",
    "    # Compute epsilon\n",
    "    orders = [1 + x / 10. for x in range(1, 100)] + list(range(11, 101))\n",
    "    sampling_probability = batch_size / n\n",
    "    rdp = compute_rdp(q=sampling_probability,\n",
    "                    noise_multiplier=noise_multiplier,\n",
    "                    steps=epochs * n // batch_size,\n",
    "                    orders=orders)\n",
    "    eps = get_privacy_spent(orders, rdp, target_delta=delta)\n",
    "    \n",
    "    # Instantiate network\n",
    "    model = create_dp_cnn(noise_multiplier, l2_norm_clip, microbatches)\n",
    "\n",
    "    # Train network\n",
    "    start_time = time.time()\n",
    "    r = model.fit(X_train, \n",
    "                 y_train, \n",
    "                 validation_data=(X_test, y_test), \n",
    "                 epochs=epochs, \n",
    "                 batch_size=batch_size,\n",
    "                # callbacks=[callback]\n",
    "                )\n",
    "    end_time = time.time()\n",
    "    time_elapsed = (end_time - start_time)\n",
    "\n",
    "    # MIA\n",
    "    aauc = []\n",
    "    aadv = []\n",
    "    for _ in range(attacks):\n",
    "        auc, adv = membership_inference_attack(model, X_train, X_test, y_train, y_test)\n",
    "        aauc.append(auc)\n",
    "        aadv.append(adv)\n",
    "    mauc = sum(aauc) / attacks\n",
    "    madv = sum(aadv) / attacks\n",
    "\n",
    "    # Write result summary\n",
    "    summ = ', '.join(map(str,[\n",
    "          len(r.history['loss']),\n",
    "          e,\n",
    "          delta,\n",
    "          l2_norm_clip,\n",
    "          noise_multiplier,\n",
    "          sampling_rate,\n",
    "          eps[0],\n",
    "          r.history['loss'][-1], \n",
    "          r.history['val_loss'][-1],\n",
    "          r.history['accuracy'][-1],\n",
    "          r.history['val_accuracy'][-1],\n",
    "          time_elapsed,\n",
    "          mauc,\n",
    "          madv\n",
    "    ]))\n",
    "    results_summary.append(summ)\n",
    "    print('='*40)\n",
    "    \n",
    "print('Epochs, Target epsilon, delta, C, Sigma, Sampling rate, Epsilon, Loss, Val loss, Accuracy, Val accuracy, Time, AUC, Advantage')\n",
    "for r in results_summary:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
